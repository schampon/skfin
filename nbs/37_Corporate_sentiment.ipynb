{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporate sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from skfin.plot import bar, line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main papers: \n",
    "- Loughran and McDonald (2011): \"When is a Liability not a Liability? Textual Analysis, Dictionaries and 10-Ks,\" *Journal of Finance*\n",
    "- Jegadeesh and Wu (2013): \"Word Power: A New Approach for Content Analysis,\" *Journal of Financial Economics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loughran-McDonalds (2011): textual analysis in finance/accounting to examine the tone and sentiment of corporate 10-K reports.\n",
    "\n",
    "- A Form 10-K is an annual report required by the U.S. Securities and Exchange Commission (SEC), that gives a comprehensive summary of a company's financial performance.\n",
    "\n",
    "Two statements: \n",
    "\n",
    "1. a Finance-specific dictionary of negative words matters\n",
    "1. weighting (e.g. tf.idf weights) matters\n",
    "\n",
    "\n",
    " Bag of words method: parse the 10-K documents into vectors of words and word counts.\n",
    " \n",
    "- Dictionaries: http://www3.nd.edu/~mcdonald/Word_Lists.html: \n",
    "- sentiment negative and positive words\n",
    "- uncertainty (e.g. approximate, contingency, depend, fluctuate, indefinite, uncertain, and variability)\n",
    "- litigation (e.g. claimant, deposition, interlocutory, testimony, and tort.)\n",
    "- modal words are related to levels of confidence: strong modal words (e.g. always, highest, must, and will) and weak model words (e.g. could, depending, might)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the most frequent sentiment words in the full 10-K document in the \"Management Discussion and Analysis\" subsection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image('images/loughran_table3.png',width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows that sentiment words reprenet about 1% of all words. In these sections, there are more words deemed negative (mean=1.39%) than positive (mean=0.75%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image('images/l7_loughran.PNG',width=350))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the predictibility of sentiment for 4-day forward value-weighted excess return (1994-2008). More precisely, the `Fin-Neg` (negative) sentiment predicts returns with a t-stat from -2.64 to -3.11 after controlling for risk factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image('images/loughran_table4.png',width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the same results for several word dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image('images/loughran_table6.png',width=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning-based sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jegadessh and Wu (2013) discuss how to fit word weights to better identify terms that drive returns.\n",
    "\n",
    "- The idea is to identify sentiment words associated to significant return moves (either positive or negative) when firm file 10Ks with the SEC. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning for document $d$: \n",
    "$$\n",
    "r_{d,t\\rightarrow t+3} = a + \\sum_{v \\in LM} b_v \\frac{count_{d,v}}{length_v} + e \n",
    "$$\n",
    "where the terms $v$ are in the base sentiment vocabulary $LM$ from  Loughran and McDonalds. \n",
    "\n",
    "Out-of-sample forecast: \n",
    "$$\n",
    "Score_d = \\sum_v \\left(\\frac{b_v - \\bar{b}}{\\sqrt{Var(b_j)}} \\right) \\frac{cound_{d,v}}{length_d}\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_{d,t+5\\rightarrow t+w} = \\alpha + \\beta Score_d  + \\epsilon \n",
    "$$\n",
    "where the statistical significance of $\\beta$ is evaluated using Fama-MacBeth statistics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the most impactful words from the regressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image('images/Jegadeesh_1.PNG',width=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows taht the words identified in the regressions are not the same as the one with high `tfidf` weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image('images/Jegadeesh_2.PNG',width=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows that the learned sentiment (as the `WP` or word power score) predicts 4-day forward returns -- even after controlling for known risk factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide \n",
    "display(Image('images/Jegadeesh_results.png',width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the 10-K/10-Q summary file from the McDonalds data repository to test some insights from the Loughran-McDonalds paper. The sentiment metric is: \n",
    "- sentiment = (#positive - #negative)/#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfin.metrics import sharpe_ratio\n",
    "from skfin.datasets import load_sklearn_stock_returns, load_10X_summaries, mapping_10X\n",
    "ret = load_sklearn_stock_returns(cache_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_10X_summaries()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_func = lambda x: (x.N_Positive -x.N_Negative)/x.N_Words\n",
    "\n",
    "sent = pd.concat({k: df.loc[lambda x: x.CoName.isin(v if isinstance(v, list) else [v])]\\\n",
    "                            .set_index('date')\\\n",
    "                            .loc['2002-01-01':ret.index[-1]]\\\n",
    "                        .pipe(sentiment_func) \n",
    "                  for k, v in mapping_10X.items()}).groupby(level=[0, 1]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sentiment metrics, we follows the steps to contruct a predictor: \n",
    "- take the sentiment difference from on filing to the previous to measure improvement or deterioration (and remove biases)\n",
    "- forward-fill the value for one-month (=21 business days) to have an active position over a limited window\n",
    "- remove the cross-sectional mean (and standardise) so that the predictor have long-short risk-managed positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sent.sort_index(level=[0, 1]).groupby(level=0).transform('diff')\\\n",
    "            .unstack(level=0).resample('B').last()\\\n",
    "            .ffill(limit=21)\\\n",
    "            .pipe(lambda x: x.sub(x.mean(axis=1), axis=0).div(x.std(axis=1), axis=0))\\\n",
    "            .reindex(ret.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pred.shift(2).mul(ret).sum(axis=1), cumsum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note that over this period, two firms contribute disportionately to the pnl -- Apple and Goldman Sachs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shift(2).mul(ret).dropna(how='all', axis=1).sum()\\\n",
    "    .pipe(lambda x: pd.concat([x.nlargest(), x.sort_values(ascending=False).tail(5)]))\\\n",
    "    .rename('Stock-level pnl contribution')\\\n",
    "    .to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the alpha decay, the graph below shows the sharpe ratio when the predictor is lagged (or led) by multiple business days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar({i: pred.shift(2+i).mul(ret).sum(axis=1).pipe(sharpe_ratio) for i in range(-10, 10)}, \n",
    "    sort=False, baseline=0, title='Lead-lag sharpe ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test different sentiment construct as shown in the graph below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "pnls_ = {}\n",
    "for c in [ 'N_Litigious',  'N_Constraining', 'N_Words']: \n",
    "    sentiment_func_ = lambda x: (x.N_Positive -x.N_Negative)/x[c]\n",
    "    sent_ = pd.concat({k: df.loc[lambda x: x.CoName.isin(v if isinstance(v, list) else [v])]\\\n",
    "                                .set_index('date')\\\n",
    "                                .loc['2002-01-01':ret.index[-1]]\\\n",
    "                            .pipe(sentiment_func_) \n",
    "                      for k, v in mapping_10X.items()}).groupby(level=[0, 1]).mean()\n",
    "\n",
    "    pred_ = sent_.sort_index(level=[0, 1]).groupby(level=0).transform('diff').unstack(level=0).resample('B').last()\\\n",
    "        .ffill(limit=21).pipe(lambda x: x.sub(x.mean(axis=1), axis=0).div(x.std(axis=1), axis=0)).reindex(ret.index)\n",
    "\n",
    "    pnls_[f'sent/{c}'] = pred_.shift(2).mul(ret).sum(axis=1)\n",
    "    \n",
    "\n",
    "for c in  ['N_Negative', 'N_Negation', 'N_WeakModal']: \n",
    "    sentiment_func_ = lambda x: -1 * x[c]/x.N_Words\n",
    "    sent_ = pd.concat({k: df.loc[lambda x: x.CoName.isin(v if isinstance(v, list) else [v])]\\\n",
    "                                .set_index('date')\\\n",
    "                                .loc['2002-01-01':ret.index[-1]]\\\n",
    "                            .pipe(sentiment_func_) \n",
    "                      for k, v in mapping_10X.items()}).groupby(level=[0, 1]).mean()\n",
    "\n",
    "    pred_ = sent_.sort_index(level=[0, 1]).groupby(level=0).transform('diff').unstack(level=0).resample('B').last()\\\n",
    "        .ffill(limit=21).pipe(lambda x: x.sub(x.mean(axis=1), axis=0).div(x.std(axis=1), axis=0)).reindex(ret.index)\n",
    "\n",
    "    pnls_[f'-1*{c}/N_word'] = pred_.shift(2).mul(ret).sum(axis=1)\n",
    "\n",
    "\n",
    "    \n",
    "for c in  ['N_Unique_Words','N_Positive', 'N_Uncertainty','N_StrongModal',  'N_Constraining']: \n",
    "    sentiment_func_ = lambda x: x[c]/x.N_Words\n",
    "    sent_ = pd.concat({k: df.loc[lambda x: x.CoName.isin(v if isinstance(v, list) else [v])]\\\n",
    "                                .set_index('date')\\\n",
    "                                .loc['2002-01-01':ret.index[-1]]\\\n",
    "                            .pipe(sentiment_func_) \n",
    "                      for k, v in mapping_10X.items()}).groupby(level=[0, 1]).mean()\n",
    "\n",
    "    pred_ = sent_.sort_index(level=[0, 1]).groupby(level=0).transform('diff').unstack(level=0).resample('B').last()\\\n",
    "        .ffill(limit=21).pipe(lambda x: x.sub(x.mean(axis=1), axis=0).div(x.std(axis=1), axis=0)).reindex(ret.index)\n",
    "\n",
    "    pnls_[f'{c}/N_word'] = pred_.shift(2).mul(ret).sum(axis=1)\n",
    "line(pnls_, cumsum=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (skfin)",
   "language": "python",
   "name": "skfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
