{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9bfa7e-df5c-4de2-9f20-0fb3a6d0e875",
   "metadata": {},
   "source": [
    "# Eigenvalue Decomposition and Risk Bias in Portfolio Covariance Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a02d8-ccf5-4a45-899c-7ed1f1293e5b",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:28.102838Z",
     "iopub.status.busy": "2025-09-29T15:44:28.102406Z",
     "iopub.status.idle": "2025-09-29T15:44:30.215990Z",
     "shell.execute_reply": "2025-09-29T15:44:30.215337Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from skfin.backtesting import Backtester\n",
    "from skfin.datasets_ import load_kf_returns\n",
    "from skfin.estimators import MLPRegressor, MultiOutputRegressor, RidgeCV\n",
    "from skfin.metrics import sharpe_ratio\n",
    "from skfin.mv_estimators import MeanVariance\n",
    "from skfin.plot import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "returns_data = load_kf_returns(filename=\"38_Industry_Portfolios\", cache_dir=\"data\")\n",
    "ret = returns_data[\"Monthly\"][\"Average_Value_Weighted_Returns\"][:\"1999\"].replace(\n",
    "    -99.99, np.nan\n",
    ")\n",
    "\n",
    "transform_X = lambda x: x.rolling(12).mean().fillna(0)\n",
    "transform_y = lambda x: x.shift(-1)\n",
    "features = transform_X(ret)\n",
    "target = transform_y(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f6c5d-6d2a-4bd7-af9a-908efa846a28",
   "metadata": {},
   "source": [
    "This chapter explores quantitative approaches to measuring financial risk, with a focus on techniques for estimating and evaluating the risk in return distributions. Through analysis of covariance matrices, eigenvalue decomposition, and out-of-sample testing, it highlights both the capabilities and limitations of common risk measurement methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd12441-43fe-405b-8062-e4f448ff05e0",
   "metadata": {},
   "source": [
    "## Risk bias metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b16cfa-a90a-4ca7-a9be-a47858efd40f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "For a given portfolio $h_{\\Phi}$ using the covariance $V_{\\Phi}$, the metric that we use is the `risk-bias` given by \n",
    "\n",
    "$$ \\text {RiskBias}_{\\Phi}  = Std \\left[\\frac{h_{\\Phi}^T r}{\\sqrt{h^T V_{\\Phi} h }} \\right] -1 , $$\n",
    "where the variance is evaluated over empirical returns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e54971-68c3-46ec-a0b8-49f8c7f72d1b",
   "metadata": {},
   "source": [
    "The returns $r_t$ are over $N$ assets (so that $r_t$ is a vector of size $N$). The covariance of returns is: \n",
    "$$V = Var(r_t).$$ \n",
    "\n",
    "Performing a singular value decomposition of $V$ yields: \n",
    "\n",
    "$$ V = U S U^T,$$\n",
    "\n",
    "where $U$ contains the eigenvectors and $S$ is a diagonal matrix with the eigenvalues. Each eigenvector can be considered as the weight of factor and we construct the factor returns as: \n",
    "\n",
    "$$ f_t = U^T r_t.$$ \n",
    "\n",
    "**Lemma**: the factors $f_t$ are uncorrelated and their volatility is by the eigenvalue. \n",
    "\n",
    "*Proof*. $$ Var(f_t) = U^T Var(r_t) U = U^T U S U^T U = S.$$\n",
    "\n",
    "**Lemma**: the eigenvector associated to the largest eigenvalue maximizes $u^T V u$ such that $u^T u = 1$. \n",
    "\n",
    "*Proof*. Introducing the Lagrange multiplier $\\xi$ on the constraint, the first-order condition is \n",
    "\n",
    "$$ V u = \\xi u, $$\n",
    "\n",
    "so that $u$ is an eigenvector and the value of the objective is the eigenvalue associated to $u$. So the objective is maximized for the largest eigenvalue. \n",
    "\n",
    "\n",
    "**Corollary**:  the eigenvector associated to the smallest eigenvalue minimizes $u^T V u$ such that $u^T u = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0d5b6-e511-4082-83bd-fe22a849d811",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Return covariance eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f752a-75e9-4aca-939c-7ecca2a2f944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:30.218546Z",
     "iopub.status.busy": "2025-09-29T15:44:30.218162Z",
     "iopub.status.idle": "2025-09-29T15:44:30.448896Z",
     "shell.execute_reply": "2025-09-29T15:44:30.448337Z"
    }
   },
   "outputs": [],
   "source": [
    "T = 60\n",
    "X = ret.iloc[:T].dropna(how=\"all\", axis=1)\n",
    "bar(X.std().sort_values(), horizontal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c335f5-7b57-47f0-8bc5-b76fed8b530b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:30.450890Z",
     "iopub.status.busy": "2025-09-29T15:44:30.450635Z",
     "iopub.status.idle": "2025-09-29T15:44:30.478135Z",
     "shell.execute_reply": "2025-09-29T15:44:30.477670Z"
    }
   },
   "outputs": [],
   "source": [
    "u, s, _ = np.linalg.svd(X.cov())\n",
    "mode_returns = X.dot(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b9f59-afd6-409d-86fc-23d94506da94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:30.479843Z",
     "iopub.status.busy": "2025-09-29T15:44:30.479625Z",
     "iopub.status.idle": "2025-09-29T15:44:31.232795Z",
     "shell.execute_reply": "2025-09-29T15:44:31.232242Z"
    }
   },
   "outputs": [],
   "source": [
    "line(mode_returns, cumsum=True, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83141a-4afd-49f5-bd14-503eb7040ee1",
   "metadata": {},
   "source": [
    "The graph below shows that the largest eigenvalue is multiple-order of magnitude larger than the smallest one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5f7ed-c507-468b-9aef-d39807b4a5e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:31.234768Z",
     "iopub.status.busy": "2025-09-29T15:44:31.234515Z",
     "iopub.status.idle": "2025-09-29T15:44:31.525661Z",
     "shell.execute_reply": "2025-09-29T15:44:31.525090Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.Series(s, np.arange(1, X.shape[1] + 1))\n",
    "scatter(\n",
    "    df,\n",
    "    xscale=\"log\",\n",
    "    yscale=\"log\",\n",
    "    xlabel=\"Eigenvalue (log scale)\",\n",
    "    ylabel=\"Rank (log scale)\",\n",
    "    xticks=[1, 2, 4, 8, 16],\n",
    "    yticks=[0.1, 1, 10, 100],\n",
    "    title=\"Distribution of return covariance eigenvalues\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d0f45-b8c4-4668-a3e4-5b89001615cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:31.527678Z",
     "iopub.status.busy": "2025-09-29T15:44:31.527396Z",
     "iopub.status.idle": "2025-09-29T15:44:31.556514Z",
     "shell.execute_reply": "2025-09-29T15:44:31.555997Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The ratio of the largest to the smallest eigenvalue is {s[0] / s[-1]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3ca6e-ffdc-4f0b-b2fb-e09c8a53a57e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:31.558382Z",
     "iopub.status.busy": "2025-09-29T15:44:31.558123Z",
     "iopub.status.idle": "2025-09-29T15:44:32.190818Z",
     "shell.execute_reply": "2025-09-29T15:44:32.190227Z"
    }
   },
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"largest eigenvector\": pd.Series(u[:, 0] / np.sign(np.mean(u[:, 0])), X.columns),\n",
    "    \"smallest eigenvalue\": pd.Series(u[:, -1] / np.sign(np.mean(u[:, 1])), X.columns),\n",
    "}\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "fig.suptitle(\"Eigvectors\", y=0.95)\n",
    "for i, (k, v) in enumerate(d.items()):\n",
    "    bar(v, title=k, ax=ax[i], horizontal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420656b-004d-467b-830e-70ba67466ea1",
   "metadata": {},
   "source": [
    "The Lemma and Corollary above show that the eigenvalues measure the *in-sample* variance of a mode. But how well does the in-sample variance predicts the out-of-sample variance? \n",
    "\n",
    "To test assess this point, we construct the pnls of modes (defined as the portfolio with the eigenvectors as positions), normalized by the ex-ante standard deviation (as the square-root of the eigenvalue) and signed so that the in-sample pnl is positive.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c11403-d484-4956-997b-864d47928f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:32.192852Z",
     "iopub.status.busy": "2025-09-29T15:44:32.192590Z",
     "iopub.status.idle": "2025-09-29T15:44:32.221209Z",
     "shell.execute_reply": "2025-09-29T15:44:32.220725Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "start_date = \"1945-01-01\"\n",
    "test_size = 1\n",
    "params = dict(max_train_size=T, test_size=test_size, gap=0)\n",
    "params[\"n_splits\"] = 1 + len(ret.loc[start_date:]) // test_size\n",
    "\n",
    "cv = TimeSeriesSplit(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be95999-c2b9-4a2d-b3d0-053a617eaffa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:32.223129Z",
     "iopub.status.busy": "2025-09-29T15:44:32.222755Z",
     "iopub.status.idle": "2025-09-29T15:44:33.788956Z",
     "shell.execute_reply": "2025-09-29T15:44:33.788315Z"
    }
   },
   "outputs": [],
   "source": [
    "mode_pnl = []\n",
    "for train, test in cv.split(ret):\n",
    "    V_ = ret.fillna(0).iloc[train].cov()\n",
    "    u, s, _ = np.linalg.svd(V_)\n",
    "    retain_max = np.sum([v > 1e-5 for v in s])\n",
    "    s[retain_max:] = np.nan * s[retain_max:]\n",
    "    mu = ret.iloc[train].fillna(0).dot(u).mean()\n",
    "    mode_pnl += [ret.fillna(0).iloc[test].dot(u).mul(np.sign(mu)).div(np.sqrt(s))]\n",
    "mode_pnl = pd.concat(mode_pnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c434672-1eb9-401b-b111-e91bc402a827",
   "metadata": {},
   "source": [
    "The graph below shows the out-of-sample risk of each mode pnl which has been rescaled to unit ex-ante risk (so that the natural baseline is 1). This metric is called a `risk bias` and will be defined formally in the next section. We see that for the first largest modes, the risk bias is close to 1, so that the ex-ante risk measures well the out-of-sample risk. However, for the smallest modes, this ex-ante meausre is completely off. For the smallest modes, the positions \"overfit\" information from the covariance matrix (in particular the correlation) and it is intuitive that the small in-sample risk estimates mean-revert to larger out-of-sample volatility.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc493906-9b63-493f-b36b-c584bff4b603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:33.791338Z",
     "iopub.status.busy": "2025-09-29T15:44:33.791074Z",
     "iopub.status.idle": "2025-09-29T15:44:34.007326Z",
     "shell.execute_reply": "2025-09-29T15:44:34.006812Z"
    }
   },
   "outputs": [],
   "source": [
    "bar(mode_pnl.std(), sort=False, title=\"Covariance mode risk bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13a2b4-e31a-4c52-af2f-06cf751cdf8e",
   "metadata": {},
   "source": [
    "## The Marcenko-Pastur pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1489835-50f4-41e6-be6c-65a09ce3136f",
   "metadata": {},
   "source": [
    "The Marcenko-Pastur probability density function (pdf) describes the theoretical distribution of eigenvalues of large random covariance matrices, which is particularly useful in the context of financial risk analysis. It defines the range and shape of the eigenvalue spectrum under certain assumptions, allowing one to distinguish between noise and meaningful signals in empirical covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eed8f0-3545-4893-9225-278172eed8b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:34.009321Z",
     "iopub.status.busy": "2025-09-29T15:44:34.009067Z",
     "iopub.status.idle": "2025-09-29T15:44:34.036624Z",
     "shell.execute_reply": "2025-09-29T15:44:34.036157Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def marcenko_pastur_pdf(x, q, sigma=1):\n",
    "    \"\"\"Marcenko-Pastur Probability Density Function (PDF).\"\"\"\n",
    "    b = sigma**2 * (1 + np.sqrt(1 / q)) ** 2\n",
    "    a = sigma**2 * (1 - np.sqrt(1 / q)) ** 2\n",
    "    if a < 0:\n",
    "        a = 0\n",
    "    pdf = np.zeros_like(x, dtype=float)\n",
    "\n",
    "    mask = (x >= a) & (x <= b)\n",
    "    pdf[mask] = (1 / (2 * np.pi * sigma**2 * x[mask] * q)) * np.sqrt(\n",
    "        (b - x[mask]) * (x[mask] - a)\n",
    "    )\n",
    "\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f10d0-01cf-4ec1-9a8b-29ee0bcbea23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:34.038331Z",
     "iopub.status.busy": "2025-09-29T15:44:34.038106Z",
     "iopub.status.idle": "2025-09-29T15:44:34.221473Z",
     "shell.execute_reply": "2025-09-29T15:44:34.220926Z"
    }
   },
   "outputs": [],
   "source": [
    "T, N = X.shape\n",
    "\n",
    "V_ = X.iloc[:T].cov()\n",
    "u, s, _ = np.linalg.svd(V_)\n",
    "\n",
    "plt.hist(s, bins=30, density=True, alpha=0.6, color=\"g\", label=\"Empirical histogram\")\n",
    "\n",
    "# Compute the Marcenko-Pastur distribution\n",
    "q = N / T\n",
    "x = np.linspace(s.min(), s.max(), 100)\n",
    "mp_pdf = marcenko_pastur_pdf(x, q, sigma=10)\n",
    "\n",
    "# Plot the Marcenko-Pastur distribution\n",
    "plt.plot(x, mp_pdf, color=\"blue\", label=\"Marcenko-Pastur PDF\")\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"Eigenvalue\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Empirical Histogram and Marcenko-Pastur PDF\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae84c46-99f1-4f44-b696-d915ca30f35e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Understanding covariance shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3547e-b310-4b26-9473-aef7b0ef9139",
   "metadata": {},
   "source": [
    "There are several methods to shrink a covariance matrix. A first method shrinks the sample covariance matrix $V$ toward its diagonal, reducing the influence of off-diagonal (correlation) terms, thus emphasizing variances over covariances: \n",
    "\n",
    "$$V_\\omega = \\omega \\times \\mathrm{Diag}(V) + (1 - \\omega) \\times V.$$\n",
    "\n",
    "\n",
    "A second method used by the `ShrunkCovariance` method in `sklearn`   shrinks $V$ toward a scaled identity matrix, where all variables are assumed to have equal variance and zero covariance, effectively imposing a model of equal risk and no correlation.\n",
    "\n",
    "$$V_\\omega = \\omega \\frac{\\mathrm{Tr}(V)}{N} \\times I_N + (1 - \\omega) \\times V.$$\n",
    "\n",
    "The primary difference is that the first approach retains individual variances while discarding correlations, whereas the second approach replaces both the variances and covariances with a uniform average variance across all assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d1466-fb2f-4a29-b986-d1b45c9370a0",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:34.223456Z",
     "iopub.status.busy": "2025-09-29T15:44:34.223188Z",
     "iopub.status.idle": "2025-09-29T15:44:34.262810Z",
     "shell.execute_reply": "2025-09-29T15:44:34.262348Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.covariance import ShrunkCovariance\n",
    "\n",
    "def simple_shrunk_covariance(x, shrinkage):\n",
    "    v = np.cov(x.T)\n",
    "    return shrinkage * np.diag(np.diag(v)) + (1 - shrinkage) * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19033bc0-fd67-42b9-8142-b593a22c4c33",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:34.264521Z",
     "iopub.status.busy": "2025-09-29T15:44:34.264296Z",
     "iopub.status.idle": "2025-09-29T15:44:34.859876Z",
     "shell.execute_reply": "2025-09-29T15:44:34.859318Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "S = {}\n",
    "U0 = {}\n",
    "for shrinkage in np.arange(0, 1.1, 0.1):\n",
    "    V_ = shrinkage * np.diag(np.diag(X.cov())) + (1 - shrinkage) * X.cov()\n",
    "    u, s, _ = np.linalg.svd(V_)\n",
    "    S[shrinkage] = s\n",
    "    U0[shrinkage] = u[:, 0] * np.sign(np.mean(u[:, 0]))\n",
    "S = pd.DataFrame.from_dict(S, orient=\"index\")\n",
    "U0 = pd.DataFrame.from_dict(U0, orient=\"index\").rename(\n",
    "    columns={i: c for i, c in enumerate(X.columns)}\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "line(S, title=\"Eigenvalues (x=0: no shrinkage; x=1: full shrinkage)\", ax=ax[0])\n",
    "\n",
    "line(\n",
    "    U0,\n",
    "    title=\"Loadings of first mode (x=0: no shrinkage; x=1: full shrinkage)\",\n",
    "    ax=ax[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33042b-a822-4cd2-a650-3990d0aee5a9",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-29T15:44:34.861912Z",
     "iopub.status.busy": "2025-09-29T15:44:34.861654Z",
     "iopub.status.idle": "2025-09-29T15:44:35.254303Z",
     "shell.execute_reply": "2025-09-29T15:44:35.253736Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "S = {}\n",
    "for shrinkage in np.arange(0, 1.01, 0.1):\n",
    "    V_ = ShrunkCovariance(shrinkage=shrinkage).fit(X.cov()).covariance_\n",
    "    _, s, _ = np.linalg.svd(V_)\n",
    "    S[shrinkage] = s\n",
    "S = pd.DataFrame.from_dict(S, orient=\"index\")\n",
    "\n",
    "line(\n",
    "    S,\n",
    "    title=\"Eigenvalues with ShrunkCovariance (x=0: no shrinkage; x=1: full shrinkage)\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skfin 2024",
   "language": "python",
   "name": "skfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
