{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we compare the performance of the Industry momentum derived in previous sections with other factors (e.g. Stock Momentum, Value, Size, etc.). More precisely, we expand on the Style regression introduced in the first section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfin.plot import line, heatmap, bar\n",
    "from skfin.datasets import load_kf_returns\n",
    "from skfin.mv_estimators import MeanVariance\n",
    "from skfin.backtesting import Backtester\n",
    "from statsmodels import api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Sharpe, 1992) Style Analysis is the process of determining what type of investment behaviour an investor or a money manager employs when making investment decisions\n",
    "\n",
    "Regression to determine the factor exposures $\\langle \\beta_1,... \\beta_K\\rangle$ where:\n",
    "\n",
    "$$ r = \\alpha + \\beta_1 r_1^{\\Phi} + ... + \\beta_K r_K^{\\Phi} + \\epsilon$$\n",
    "\n",
    "Additional constraints might be added to “regularize\" the regression such as non-negative exposures :\n",
    "\n",
    "$\\beta_k \\geq 0$ and/or sum equals 1: $\\sum_{k=1}^{K} \\beta_k = 1$.\n",
    "\n",
    "Frazzini, Kabiller and Pedersen (2013):\n",
    "\n",
    "> Berkshire Hathaway has realized a Sharpe ratio of 0.76, higher than any other stock or mutual fund with a history of more than 30 years, and Berkshire has a significant alpha to traditional risk factors.\"\n",
    "\n",
    "How did Warren Buffet do it?\n",
    "\n",
    "- Use a “style analysis\" approach applied to equity factors to\n",
    "address this question.\n",
    "\n",
    "The main regression is:\n",
    "$$r_t = \\alpha +\\beta_1 MKT_t +\\beta_2 SMB_t +\\beta_3 HML_t +\\beta_4 UMD_t + \\beta_5 BAB_t + \\beta QMJ_t +\\epsilon_t$$\n",
    "\n",
    "where the factors are\n",
    "\n",
    "- $r_t$ : excess return of the Berkshire Hathaway stock\n",
    "\n",
    "- $MKT_t$ (market): excess market return\n",
    "\n",
    "-  $SMB_t$ (size): small minus big\n",
    "\n",
    "-  $HML_t$ (value): high book-to-market minus low book-to-market\n",
    "\n",
    "-  $UMD_t$ (momentum): up minus down\n",
    "\n",
    "-  $BAB_t$ (betting-against-beta): safe (low beta) minus risky (high beta)\n",
    "\n",
    "- $QMJ_y$ (quality): quality minus junk\n",
    "\n",
    "Can we replicate this finding? Fortunately Steve Lihn (on GitHub) already did it.\n",
    "\n",
    "Data: github.com/slihn/buffetts_alpha_R/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image(\"images/l1_frazzini_table4heading.PNG\"))\n",
    "display(Image(\"images/l1_frazzini_table4.PNG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The characteristics of the investment of Warren Buffet: high loadings on replicable factors such as beta, size, value and quality – and a negative loading on momentum.\n",
    "\n",
    "\n",
    "- At least in this replication of the paper's results (with slightly different data), the intercept is no longer statistically significant – it might still be economically significant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfin.datasets import load_buffets_data\n",
    "data = load_buffets_data(cache_dir=\"data\").assign(excess_return=lambda x: x[\"BRK.A\"] - x[\"RF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = api.OLS(data[\"excess_return\"], api.add_constant(data[\"MKT\"])).fit()\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for cols in [\n",
    "    [\"MKT\", \"SMB\", \"HML\", \"UMD\"],\n",
    "    [\"MKT\", \"SMB\", \"HML\", \"UMD\", \"BAB\"],\n",
    "    [\"MKT\", \"SMB\", \"HML\", \"UMD\", \"BAB\", \"QMJ\"],\n",
    "]:\n",
    "    m_ = api.OLS(data[\"excess_return\"], api.add_constant(data[cols])).fit()\n",
    "    summaries += [m_.summary()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_table(tbl):\n",
    "    df = pd.DataFrame(tbl.tables[1].data)\n",
    "    idx = df.iloc[1:, 0]\n",
    "    return pd.DataFrame(\n",
    "        df.iloc[1:, [1, 3]].astype(float).values,\n",
    "        index=idx.rename(None),\n",
    "        columns=[\"coef\", \"tstat\"],\n",
    "    ).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([prettify_table(v) for v in summaries], axis=1).fillna(0).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients are qualitatively close from the results in the paper -- with the except of the `BAB` coefficients not being statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry momentum factor exposure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we go back to the Industry momentum backtest and decompose it on the factors as computed by Ken French. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_data = load_kf_returns(cache_dir='data')\n",
    "ret = returns_data['Monthly']['Average_Value_Weighted_Returns'][:'1999']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_X = lambda x: x.rolling(12).mean().fillna(0).values\n",
    "transform_y = lambda x: x.shift(-1).values\n",
    "features = transform_X(ret)\n",
    "target = transform_y(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl0 = Backtester(MeanVariance(), ret).train(features, target).pnl_\n",
    "line(pnl0, cumsum=True, title='Industry momentum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"F-F_Research_Data_Factors\", \"F-F_Momentum_Factor\"]\n",
    "df = pd.concat([load_kf_returns(c)['Monthly'] for c in files], axis=1)['1945':'1999']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(df.corrwith(pnl0), horizontal=True, title= 'Correlation with industry momentum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.join(pnl0.rename('IndustryMom'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = api.OLS(data[\"IndustryMom\"], api.add_constant(data.drop(\"IndustryMom\", axis=1))).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pd.concat({'Stock momentum': df['Mom   '], \n",
    "                'Industry momentum': pnl0}, axis=1).pipe(lambda x: x.div(x.std())), \n",
    "     cumsum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main issue with this statistical decomposition is that the estimation is done \"full sample\". In the next section, we estimate the Momentum loading on rolling windows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual pnl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the rolling estimation decomposition, we use the function `fit_predict` used in previous sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import  TimeSeriesSplit\n",
    "from skfin.backtesting import fit_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"1945-01-01\"\n",
    "max_train_size = 60\n",
    "test_size = 1\n",
    "params = dict(max_train_size=max_train_size, test_size=test_size, gap=0)\n",
    "params[\"n_splits\"] =  (len(data) - max_train_size) // test_size\n",
    "cv_ = TimeSeriesSplit(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl_hat, estimator_ = fit_predict(estimator = LinearRegression(), \n",
    "                      features = data.drop([\"IndustryMom\", \"RF\"], axis=1), \n",
    "                      target = data[\"IndustryMom\"], \n",
    "                      ret = data[\"IndustryMom\"].to_frame(),\n",
    "                      cv = cv_, return_estimator=True)\n",
    "pnl_hat = pnl_hat.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line({'pnl0': pnl0[max_train_size:], 'predict': pnl_hat.squeeze(), \n",
    "      'residue': pnl0[max_train_size:] - pnl_hat}, cumsum=True, title='Rolling residual decomposition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(20, 5))\n",
    "line(pd.DataFrame([m.intercept_ for m in estimator_], index=pnl_hat.index), \n",
    "     title='Intercept coefficient', ax=ax[0], legend=False)\n",
    "\n",
    "line(pd.DataFrame([m.coef_ for m in estimator_], \n",
    "                  columns=data.drop(['IndustryMom', 'RF'], axis=1).columns, \n",
    "                  index=pnl_hat.index), \n",
    "     title='Slope coefficients', loc='best', ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over this period, the simple Industry momentum strategy seems to have zero residual relative to other factors. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (skfin)",
   "language": "python",
   "name": "skfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
