{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "\n",
    "from skfin.plot import line, bar\n",
    "from skfin.datasets import load_kf_returns\n",
    "from skfin.mv_estimators import MeanVariance \n",
    "from skfin.backtesting import Backtester\n",
    "from skfin.metrics import sharpe_ratio\n",
    "from skfin.estimators import RidgeCV, MultiOutputRegressor, MLPRegressor\n",
    "\n",
    "returns_data = load_kf_returns(cache_dir='data')\n",
    "ret = returns_data['Monthly']['Average_Value_Weighted_Returns'][:'1999']\n",
    "\n",
    "transform_X = lambda x: x.rolling(12).mean().fillna(0).values\n",
    "transform_y = lambda x: x.shift(-1).values\n",
    "features = transform_X(ret)\n",
    "target = transform_y(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than choosing a single estimator (or set of parameters) among many, another stategy is to combine all the possible estimators/parameters. `scikit-learn` allows to do that with classes such as `VotingRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfin.estimators import Ridge\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "class VotingRegressor(VotingRegressor):\n",
    "    def transform(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_ = [('ridge1', Ridge(alpha=1)),  \n",
    "               ('ridge2', Ridge(alpha=100)), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `VotingRegressor` applies equal weights across regressors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(StandardScaler(with_mean=False), \n",
    "                          MultiOutputRegressor(VotingRegressor(estimators=estimators_)), \n",
    "                          MeanVariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Backtester(estimator, ret).train(features, target)\n",
    "line(m.pnl_, cumsum=True, title='Voting regressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `scikit-learn`, there is also a `StackingRegressor` but it requires a bit more work to make it work with `MultiOutputRegressor` (and constraints on transform/regressors). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling ensemble backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we build a custom ensemble method to learn weights on different estimators from pnls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StackingBacktester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we consider three estimators: \n",
    "    \n",
    "- the simple Industry momentum. \n",
    "\n",
    "- a strategy that learns cross-industry effect with `Ridge`. \n",
    "\n",
    "- a strategy that learns cross-industry effect with `Lightgbm`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {'momentum': MeanVariance(), \n",
    "              'ridge':  make_pipeline(StandardScaler(with_mean=False), Ridge(), MeanVariance()), \n",
    "              'lightgbm': make_pipeline(MultiOutputRegressor(LGBMRegressor(min_child_samples=5, \n",
    "                                                             n_estimators=25, n_jobs=1)), MeanVariance())\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls = pd.concat({k: Backtester(v, ret).train(features, target).pnl_ for k, v in estimators.items()}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls_ = pnls.assign(equal_weight = lambda x: x.sum(axis=1).div(np.sqrt(x.shape[1])))\n",
    "line(pnls_, cumsum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average correlation is not particularly high, which explains with some simple ensemble seems to help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The average pnl correlation between estimators is {pnls.corr().stack().loc[lambda x: x!=1].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a `StackingBacktester` with the `sklearn` api. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../skfin/ensemble.py\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from skfin.mv_estimators import Mbj \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "    \n",
    "\n",
    "class StackingBacktester:\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimators,\n",
    "        ret,\n",
    "        max_train_size=36,\n",
    "        test_size=1,\n",
    "        start_date=\"1945-01-01\",\n",
    "        end_date=None,\n",
    "        window=60, \n",
    "        min_periods=60, \n",
    "        final_estimator = Mbj()\n",
    "    ):\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.estimators = estimators\n",
    "        self.ret = ret[: self.end_date]\n",
    "        self.cv = TimeSeriesSplit(\n",
    "            max_train_size=max_train_size,\n",
    "            test_size=test_size,\n",
    "            n_splits=1 + len(ret.loc[start_date:end_date]) // test_size,\n",
    "        )\n",
    "        self.window = window\n",
    "        self.min_periods = min_periods\n",
    "        self.final_estimator = final_estimator \n",
    "\n",
    "    def train(self, features, target):\n",
    "        cols =self.ret.columns \n",
    "        idx = self.ret.index[np.concatenate([test for _, test in self.cv.split(self.ret)])]\n",
    "\n",
    "        _h = {k: [] for k in list(self.estimators.keys()) + ['ensemble']}\n",
    "        _pnls = {k: [] for k in self.estimators.keys()}\n",
    "        _coef = []\n",
    "        for i, (train, test) in enumerate(self.cv.split(self.ret)): \n",
    "            h_ = {}\n",
    "            if (i> self.min_periods): \n",
    "                pnl_window = np.stack([np.array(v[-self.window:]) for k, v in _pnls.items()], axis=1)\n",
    "                coef_ = self.final_estimator.fit(pnl_window).coef_\n",
    "                _coef += [coef_]\n",
    "            else: \n",
    "                _coef += [np.zeros(3)] \n",
    "            for k, m in self.estimators.items(): \n",
    "                m.fit(features[train], target[train])\n",
    "                h_[k] = m.predict(features[test])\n",
    "                _h[k] += [h_[k]]\n",
    "                if i+1 <len(idx):\n",
    "                    _pnls[k] += [self.ret.loc[idx[i+1]].dot(np.squeeze(h_[k]))]\n",
    "            if (i>self.min_periods): \n",
    "                h_ensemble = np.stack([np.squeeze(v) for v in h_.values()], axis=1).dot(coef_).reshape(-1, 1)\n",
    "                V_ = m.named_steps['meanvariance'].V_\n",
    "                h_ensemble = h_ensemble / np.sqrt(np.diag(h_ensemble.T.dot(V_.dot(h_ensemble))))\n",
    "            else: \n",
    "                h_ensemble = np.zeros([len(cols), 1])\n",
    "            _h['ensemble'] += [h_ensemble.T]\n",
    "            \n",
    "        self.h_ = {k: pd.DataFrame(np.concatenate(_h[k]), index=idx, columns=cols) \n",
    "                   for k in _h.keys()}\n",
    "        self.pnls_ = pd.concat({k: v.shift(1).mul(self.ret).sum(axis=1)[self.start_date:] \n",
    "                                for k, v in self.h_.items()}, \n",
    "                               axis=1)\n",
    "        self.coef_ = pd.DataFrame(np.stack(_coef), index=idx, columns=self.estimators.keys())\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfin.mv_estimators import Mbj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mbj()\n",
    "m.fit(pnls)\n",
    "bar(pd.Series(m.coef_, index=pnls.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in-sample optimal weights improve even more the sharpe ratio -- but this is `in-sample`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pnls_.assign(in_sample_optimal = Mbj().fit_transform(pnls)), cumsum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StackingBacktester` computes the performance with the `MBJ` learned weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfin.ensemble import StackingBacktester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StackingBacktester(estimators=estimators, \n",
    "                       ret=ret, window=60,min_periods=60).train(features, target)\n",
    "pnls = pnls.assign(ensemble_mbj= m.pnls_['ensemble'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(m.pnls_['1950-02-01':], cumsum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why the performance is lower, it is useful to look at the weights -- in this case, the weights are often negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(m.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redo the exercise with a positive-weight constraint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StackingBacktester(estimators=estimators, \n",
    "                       final_estimator=Mbj(positive=True), \n",
    "                       ret=ret, \n",
    "                       window=60,\n",
    "                       min_periods=60)\n",
    "m.train(features, target)\n",
    "pnls['ensemble_mbj_positive'] = m.pnls_['ensemble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "line(m.pnls_['1950-02-01':], cumsum=True, ax=ax[0], loc='best')\n",
    "line(m.coef_, ax=ax[1], loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over longer periods with positive constraints, the performance is closer to the industry momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StackingBacktester(estimators=estimators, \n",
    "                       final_estimator=Mbj(positive=True), \n",
    "                       ret=ret, window=180,min_periods=60)\n",
    "\n",
    "m.train(features, target)\n",
    "pnls['ensemble_mbj_positive_long_window'] = m.pnls_['ensemble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "line(m.pnls_, cumsum=True, ax=ax[0], loc='best')\n",
    "line(m.coef_, ax=ax[1], loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the different ensembles, we compare the pnls in the graph below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pnls['1950-02-01':], cumsum=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (skfin)",
   "language": "python",
   "name": "skfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
