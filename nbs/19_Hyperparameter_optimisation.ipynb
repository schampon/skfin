{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we cover the topic of `overfitting`. A folk theorem in asset management is that people are so afraid of overfitting that they tend to (massively) underfit. Or at least, that was the case. Today, better fitting models to extract as much information from a dataset has become a crucial skill. \n",
    "\n",
    "More precisely, `overfitting` a particular dataset provides a baseline for how well a system can learn (e.g. see the Recipe for Training Neural Nets by Andrej Karpathy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "\n",
    "from skfin.plot import line, bar\n",
    "from skfin.datasets import load_kf_returns\n",
    "from skfin.mv_estimators import MeanVariance \n",
    "from skfin.backtesting import Backtester\n",
    "from skfin.metrics import sharpe_ratio\n",
    "from skfin.estimators import RidgeCV, MultiOutputRegressor, MLPRegressor\n",
    "\n",
    "returns_data = load_kf_returns(cache_dir='data')\n",
    "ret = returns_data['Monthly']['Average_Value_Weighted_Returns'][:'1999']\n",
    "\n",
    "transform_X = lambda x: x.rolling(12).mean().fillna(0).values\n",
    "transform_y = lambda x: x.shift(-1).values\n",
    "features = transform_X(ret)\n",
    "target = transform_y(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfin.datasets import load_kf_returns\n",
    "returns_data = load_kf_returns(cache_dir='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy is to use estimators that embed some form of cross-validation like `RidgeCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation is described as follows: \n",
    "\n",
    "Take a model with parameter $s$ (e.g. the Ridge with tuning parameter `alpha`): \n",
    "\n",
    "1. divide the data into $K$ roughly equal parts ($K = 5$ or $K = 10$)\n",
    "\n",
    "1. for each $k \\in \\{1, 2,..., K\\}$ fit the model with parameter $s$ to the other $K-1$ parts and compute its error $E_k(s)$  in predicting the $k$-th part.\n",
    "\n",
    "1. the overall cross-validation error is then $CV(s)= \\frac{1}{K} \\sum_{k=1}^K E_k(s)$. \n",
    "\n",
    "1. do this for many values of $s$ and choose the value of s that minimize $CV (s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.exp(np.arange(np.log(10),np.log(10001), (np.log(10000) - np.log(10))/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(StandardScaler(with_mean=False), \n",
    "                          PolynomialFeatures(degree=2), \n",
    "                          RidgeCV(alphas=alphas, cv=5), \n",
    "                          MeanVariance())\n",
    "\n",
    "m = Backtester(estimator, ret).train(features, target)\n",
    "line(m.pnl_, cumsum=True, title='RidgeCV with polynomial features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pd.Series([m[2].alpha_ for m in m.estimators_]), title='Cross-validated alphas', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the fitted alphas over rolling windows are not very stable (probably given the small rolling windows used here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random parameter search for Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lightgbm.readthedocs.io/en/latest/Parameters.html#learning-control-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute a `Lightgbm` benchmark with the fixed baseline parameters used in a previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(MultiOutputRegressor(LGBMRegressor(min_child_samples=5, \n",
    "                                                             n_estimators=25, \n",
    "                                                             n_jobs=1)), \n",
    "                          MeanVariance())\n",
    "\n",
    "pnl_lgb = {'fixed_params':  Backtester(estimator, ret).train(features, target).pnl_ }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do a search with random parameters drawn from predetermined distribution: \n",
    "\n",
    "- the random parameter generators come from the `scipy.stats` module -- in particular `randint`, `uniform` and `loguniform`.\n",
    "- we use the `scikit-learn` function `ParameterSampler` as wrapper. \n",
    "\n",
    "Setup: \n",
    "\n",
    "- the objective is to maximize the sharpe ratio over the early period 1945 to 1972 (as the `train` period). \n",
    "- the evaluation is the performance of the backtest over the 1972-to-2000 period (as the `test` period). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats import randint, uniform, loguniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50 \n",
    "start_date = \"1945-01-01\"\n",
    "end_date = \"1972-04-01\"\n",
    "param_distributions = {\"max_depth\": randint(3, 10),\n",
    "                       \"num_leaves\": randint(2, 2**8), \n",
    "                       \"n_estimators\": randint(5, 50), \n",
    "                       \"min_split_gain\": uniform(0, 1.0), \n",
    "                       \"min_child_samples\": randint(1, 5), \n",
    "                       \"reg_lambda\": loguniform(1e-8, 1.0), \n",
    "                       \"reg_alpha\": loguniform(1e-8, 1.0)\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_recompute = False \n",
    "cache_dir = Path(os.getcwd()) / \"cache\"\n",
    "if not cache_dir.is_dir(): \n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "fname = cache_dir /'hpo_lgb.parquet'\n",
    "if (force_recompute)|(not fname.exists()):\n",
    "    results_ = {}\n",
    "    for i, prm in enumerate(ParameterSampler(param_distributions=param_distributions, n_iter=n_iter)): \n",
    "        estimator = make_pipeline(MultiOutputRegressor(LGBMRegressor(n_jobs=1, **prm)), \n",
    "                                  MeanVariance())\n",
    "        pnl_ = Backtester(estimator, ret, end_date=end_date).train(features, target).pnl_\n",
    "        prm.update({'sr': pnl_.pipe(sharpe_ratio)})\n",
    "        results_[i] = pd.Series(prm)\n",
    "    results = pd.DataFrame.from_dict(results_, orient='index').sort_values('sr')\n",
    "    results.to_parquet(fname)\n",
    "else: \n",
    "    results = pd.read_parquet(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.sort_index()[['sr']].assign(sr_cummax = lambda x: x.sr.cummax())\n",
    "line(df, title='Optimisation history: random search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sharpe ratio statistics presented in a previous section, we can compute a standard error around the maximum sharpe ratio: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_max = results.iloc[-1]['sr'] / np.sqrt(12)\n",
    "sr_std = np.sqrt(12) * np.sqrt((1 + .5 * sr_max **2)/ len(ret[start_date:end_date])) \n",
    "sr_range = results['sr'].pipe(lambda x: x.max()-x.min())\n",
    "print(f'The sharpe ratio standard deviation at the maximum sharpe ratio (of {sr_max * np.sqrt(12):.2f}) is {sr_std:.2f}')\n",
    "\n",
    "print(f'The range of the sharpe ratios in the random search is {sr_range:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = results.drop('sr', axis=1).iloc[-1].to_dict()\n",
    "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_params['min_child_samples'] = int(best_params['min_child_samples'])\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(MultiOutputRegressor(LGBMRegressor(n_jobs=1, **best_params)), \n",
    "                          MeanVariance())\n",
    "pnl_lgb['best_params'] = Backtester(estimator, ret).train(features, target).pnl_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line({k: v.loc[start_date:end_date] for k, v in pnl_lgb.items()}, cumsum=True, title='Lightgbm search: in-sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line({k: v.loc[end_date:] for k, v in pnl_lgb.items()}, cumsum=True, title='Lightgbm search: out-of-sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the parameters that are correlated with the sharpe ratio? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(results.corr()['sr'].mul(np.sqrt(n_iter)).drop('sr'), \n",
    "    title='T-stat correlation param value / sharpe ratio', horizontal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess more precisely the impact of parameters on the sharpe ratio, we run a regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels import api\n",
    "m = api.OLS(results[\"sr\"], api.add_constant(results.drop('sr', axis=1))).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (skfin)",
   "language": "python",
   "name": "skfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
