{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d4ff7b-8075-44a3-9e0d-6b1467965bc0",
   "metadata": {},
   "source": [
    "# Leveraging Large Language Models to Generate Economic Expectations from Historical News and Fed Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156073b3-40ad-4410-96bf-35dec8522b2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:33.080943Z",
     "iopub.status.busy": "2025-09-29T15:55:33.080747Z",
     "iopub.status.idle": "2025-09-29T15:55:34.595631Z",
     "shell.execute_reply": "2025-09-29T15:55:34.595097Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "with open(\"/home/schamponn/dev/projects/llm/key\", \"r\") as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "import logging\n",
    "\n",
    "httpx_logger = logging.getLogger(\"httpx\")\n",
    "httpx_logger.setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from skfin.datasets_ import load_fomc_statements\n",
    "from skfin.text import show_text\n",
    "from skfin.plot import *\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb0fdf-749f-42d2-a8e5-42d2f41ee27e",
   "metadata": {},
   "source": [
    "This chapter introduces a novel methodology for generating economic expectations using large language models (LLMs) applied to historical news data. The central goal is to demonstrate how LLMs, specifically models like OpenAIâ€™s GPT-3.5, can process and analyze vast quantities of news articles to estimate economic sentiment and expectations. \n",
    "\n",
    "By deriving these LLM-generated expectations, we review their usefulness in understanding broader economic sentiment and the dynamics underlying behavioral theories, such as bubbles. Furthermore, these generated expectations are systematically compared to more traditional survey-based measures, highlighting the potential and implications of leveraging advanced language models in economic forecasting and sentiment analysis.\n",
    "\n",
    "In this notebook, we also illustrate how to use `Large Language Models` (LLM), in particular models by Openai to extract information for each document of a corpus. The corpus that we will use is the collection of statements by the US Federal Reserve since 1999. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be573c-b581-4a51-b15e-7338de36413d",
   "metadata": {},
   "source": [
    "## Generating estimates from news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871080d1-a1b7-4846-b3aa-a6aa4334e5c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:34.597812Z",
     "iopub.status.busy": "2025-09-29T15:55:34.597482Z",
     "iopub.status.idle": "2025-09-29T15:55:34.604490Z",
     "shell.execute_reply": "2025-09-29T15:55:34.604059Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "display(Image(\"images/bybee_title.png\", width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebefc66d-4e15-4510-a174-f61d62371c5a",
   "metadata": {},
   "source": [
    "The main objective of the paper is to introduce a methodology for generating economic expectations using large language models (LLMs) applied to historical news data, and to explore the implications of these generated expectations for understanding economic sentiment and behavioral theories of bubbles.\n",
    "\n",
    "More precisely, the research uses OpenAI's GPT-3.5, to generate economic expectations from historical news articles. The generated expectations are then compared to existing survey measures to evaluate their accuracy and used to construct a measure of economic sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b664510-3f9c-4611-886d-5b63cedcf39d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:34.606142Z",
     "iopub.status.busy": "2025-09-29T15:55:34.605935Z",
     "iopub.status.idle": "2025-09-29T15:55:34.617976Z",
     "shell.execute_reply": "2025-09-29T15:55:34.617558Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "display(Image(\"images/bybee_prompt.png\", width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5814a-89d7-4bd9-92fb-8ed89b7e10e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:34.619500Z",
     "iopub.status.busy": "2025-09-29T15:55:34.619291Z",
     "iopub.status.idle": "2025-09-29T15:55:34.638797Z",
     "shell.execute_reply": "2025-09-29T15:55:34.638354Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "display(Image(\"images/bybee_targets.png\", width=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e37ef8-1a7e-4c29-8f4f-935785cb96b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:34.640309Z",
     "iopub.status.busy": "2025-09-29T15:55:34.640106Z",
     "iopub.status.idle": "2025-09-29T15:55:34.647332Z",
     "shell.execute_reply": "2025-09-29T15:55:34.646906Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "display(Image(\"images/bybee_responses.png\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8dfca3-5e57-4048-9071-3c7a7518c94a",
   "metadata": {},
   "source": [
    "The study finds that generated expectations closely match existing survey measures and capture deviations from full-information rational expectations. The generated measure of economic sentiment is positively correlated with past returns and negatively correlated with future returns, indicating its potential as a predictor of market bubbles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933376c5-188c-4ad8-ad3e-2c14abb459a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:34.648852Z",
     "iopub.status.busy": "2025-09-29T15:55:34.648654Z",
     "iopub.status.idle": "2025-09-29T15:55:34.656377Z",
     "shell.execute_reply": "2025-09-29T15:55:34.655952Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "display(Image(\"images/bybee_correlation.png\", width=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0f15e-8e77-42f9-9c00-7ac128dde986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:34.657845Z",
     "iopub.status.busy": "2025-09-29T15:55:34.657646Z",
     "iopub.status.idle": "2025-09-29T15:55:34.666028Z",
     "shell.execute_reply": "2025-09-29T15:55:34.665609Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "display(Image(\"images/bybee_future_returns.png\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db6266-b340-48a2-b471-536354a70f09",
   "metadata": {},
   "source": [
    "## LLM wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caae59d-bd3c-4361-8b19-1d8902fdc024",
   "metadata": {},
   "source": [
    "The code below is based on the Openai package. Given that Openai only proposes commercial LLMs, you need to have created an account and been provided a key. In the cell below you can paste the key: `api_key=sk-...`. Alternatively (as done below), you can put in a file call \"key.\" and call the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3150238-6ca8-4349-a79f-fa272224dcf7",
   "metadata": {},
   "source": [
    "The following functions in `llm.py` allows to access language models by `openai`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0700b-5e83-4c52-a43b-0ae331e7df4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:34.667839Z",
     "iopub.status.busy": "2025-09-29T15:55:34.667542Z",
     "iopub.status.idle": "2025-09-29T15:55:34.676324Z",
     "shell.execute_reply": "2025-09-29T15:55:34.675876Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile ../skfin/llm.py\n",
    "import os\n",
    "import ssl\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import httpx\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "@retry(wait=wait_exponential(multiplier=1, min=6, max=10), stop=stop_after_attempt(5))\n",
    "def apply_prompt_single_text(\n",
    "    text,\n",
    "    content_function,\n",
    "    response_format,\n",
    "    nested_response=False,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_new_tokens=2048,\n",
    "    temperature=0,\n",
    "    api_key=None,\n",
    "):\n",
    "    \"\"\"Apply a specified prompt to a single text input using an OpenAI GPT model, and format the response.\n",
    "\n",
    "    This function retries on failure with an exponential backoff strategy up to 5 attempts.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be processed by the prompt.\n",
    "        content_func (callable): A function that takes a single string argument (text) and returns a formatted string.\n",
    "        response_format (ResponseFormat): An instance specifying the desired format of the response.\n",
    "        model (str, optional): The identifier for the model to use. Defaults to \"gpt-4o-mini\".\n",
    "        max_new_tokens (int, optional): The maximum number of new tokens to generate in the response. Defaults to 2048.\n",
    "        temperature (float, optional): Sampling temperature to use for the response generation. A value closer to 0 makes the output more deterministic. Defaults to 0.\n",
    "        api_key (str, optional): OpenAi key. Defaults to None, in which case, it loaded in the local folder.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys correspond to the properties specified in the response format's JSON schema,\n",
    "              and values are the parsed responses from the model. Returns 'NA' if no output is generated.\n",
    "\n",
    "    Raises:\n",
    "        openai.error.OpenAIError: If fails to get a response after the specified retry attempts.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Assume the role of a financial analyst.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_function(text),\n",
    "        },\n",
    "    ]\n",
    "    if api_key is None:\n",
    "        with open(\"key\", \"r\") as f:\n",
    "            api_key = f.read()\n",
    "    client = OpenAI(api_key=api_key, \n",
    "                    http_client=httpx.Client(verify= os.environ.get(\"REQUESTS_CA_BUNDLE\")))\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format=response_format,\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=0,\n",
    "        seed=42,\n",
    "        logprobs=True,\n",
    "        top_logprobs=10,\n",
    "        top_p=1,\n",
    "    )\n",
    "    message = completion.choices[0].message\n",
    "    if message.parsed:\n",
    "        keys = list(response_format.model_json_schema()[\"properties\"].keys())\n",
    "        output = {k: message.parsed.__dict__[k] for k in keys}\n",
    "        if nested_response:\n",
    "            assert len(output) == 1\n",
    "            output = output[list(output.keys())[0]]\n",
    "            assert isinstance(output, list)\n",
    "            k, v = output[0].model_json_schema()[\"properties\"].keys()\n",
    "            output = pd.Series({c.__dict__[k]: c.__dict__[v] for c in output})\n",
    "    else:\n",
    "        print(\"No output\")\n",
    "        output = \"NA\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def apply_prompt(\n",
    "    dataframe,\n",
    "    content_function,\n",
    "    response_format,\n",
    "    nested_response=False,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    text_column=\"text\",\n",
    "    max_workers=25,\n",
    "    sequential=False,\n",
    "    return_dataframe=True,\n",
    "    stacked_dataframe=False, \n",
    "    api_key=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply a specified prompt to a DataFrame containing text data using an OpenAI GPT model.\n",
    "\n",
    "    This function can process the text data either sequentially or concurrently using a thread pool.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing the text data to be processed.\n",
    "        content_function (callable): A function that takes a single string argument (text) and returns a formatted string.\n",
    "        response_format (ResponseFormat): An instance specifying the desired format of the response.\n",
    "        text_columns (str, optional): The column name in the DataFrame that contains the text data. Defaults to 'text'.\n",
    "        max_workers (int, optional): The maximum number of worker threads to use for concurrent execution. Defaults to 25.\n",
    "        sequential (bool, optional): If True, process the text data sequentially. If False, process the data concurrently. Defaults to False.\n",
    "        return_dataframe (bool, optional): If True, return the results as a new DataFrame. If False, return the results as a list. Defaults to True.\n",
    "        api_key (str, optional): OpenAi key. Defaults to None, in which case, it loaded in the local folder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or list: The processed output. A DataFrame if `return_dataframe` is True, otherwise a list.\n",
    "    \"\"\"\n",
    "\n",
    "    texts = dataframe[text_column].values\n",
    "\n",
    "    if sequential:\n",
    "        results = [\n",
    "            apply_prompt_single_text(\n",
    "                text,\n",
    "                content_function=content_function,\n",
    "                response_format=response_format,\n",
    "                nested_response=nested_response,\n",
    "                model=model,\n",
    "                api_key=api_key,\n",
    "            )\n",
    "            for text in texts\n",
    "        ]\n",
    "    else:\n",
    "        apply_prompt_single_text_partial = partial(\n",
    "            apply_prompt_single_text,\n",
    "            content_function=content_function,\n",
    "            response_format=response_format,\n",
    "            nested_response=nested_response,\n",
    "            model=model,\n",
    "            api_key=api_key,\n",
    "        )\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = list(executor.map(apply_prompt_single_text_partial, texts))\n",
    "\n",
    "    if return_dataframe:\n",
    "        if stacked_dataframe:\n",
    "            return pd.concat({dataframe.index[i]: pd.DataFrame(d) for i, d in enumerate(results)})\n",
    "        else:\n",
    "            return pd.DataFrame(results, index=dataframe.index)\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2c861-d0fc-4370-a34d-d14d73de5ad0",
   "metadata": {},
   "source": [
    "In the function above (and the analysis below), the baseline model is `gpt-4o-mini`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cfb529-ce52-4a21-bdc0-7a6963a9637c",
   "metadata": {},
   "source": [
    "## Measuring hawkishness in FOMC statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9551a97-0b96-4f1a-9043-8f25a9002b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:34.677850Z",
     "iopub.status.busy": "2025-09-29T15:55:34.677651Z",
     "iopub.status.idle": "2025-09-29T15:55:34.723075Z",
     "shell.execute_reply": "2025-09-29T15:55:34.722610Z"
    }
   },
   "outputs": [],
   "source": [
    "statements = load_fomc_statements(force_reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99464686-5644-46ec-883f-da255b43b9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:34.724660Z",
     "iopub.status.busy": "2025-09-29T15:55:34.724447Z",
     "iopub.status.idle": "2025-09-29T15:55:34.728041Z",
     "shell.execute_reply": "2025-09-29T15:55:34.727618Z"
    }
   },
   "outputs": [],
   "source": [
    "text = statements[\"text\"].iloc[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad770b2-6a37-4d08-b31e-ea59b8c4a71b",
   "metadata": {},
   "source": [
    "With the following prompt, we ask the model to qualify Fed statements in terms of hawkishness/dovishness. We also insist on receiving a single-word answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf4e6c-b02e-4434-b8db-864f3def3f3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:34.729592Z",
     "iopub.status.busy": "2025-09-29T15:55:34.729390Z",
     "iopub.status.idle": "2025-09-29T15:55:36.649632Z",
     "shell.execute_reply": "2025-09-29T15:55:36.649091Z"
    }
   },
   "outputs": [],
   "source": [
    "from skfin.llm import apply_prompt\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "def content_hawkishness(text):\n",
    "    return f\"\"\"Act as a financial analyst. What is the monetary policy hawkishness of this text?\n",
    "    Please choose an answer from hawkish, dovish, neutral or unknown and provide a probability and a short explanation. \n",
    "\n",
    "Text: {text}\"\"\"\n",
    "\n",
    "\n",
    "class format_hawkishness(BaseModel):\n",
    "    hawkishness: Literal[\"hawkish\", \"neutral\", \"dovish\", \"unknown\"]\n",
    "    probability: float\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d049d-6d66-4452-98a8-dd25ea20e87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:55:36.651651Z",
     "iopub.status.busy": "2025-09-29T15:55:36.651402Z",
     "iopub.status.idle": "2025-09-29T15:57:04.372590Z",
     "shell.execute_reply": "2025-09-29T15:57:04.371973Z"
    }
   },
   "outputs": [],
   "source": [
    "df = apply_prompt(\n",
    "    statements,\n",
    "    content_function=content_hawkishness,\n",
    "    response_format=format_hawkishness,\n",
    "    text_column=\"text\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cbdc5f-73c6-434d-afdc-a85c265d4f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:04.374719Z",
     "iopub.status.busy": "2025-09-29T15:57:04.374459Z",
     "iopub.status.idle": "2025-09-29T15:57:04.383005Z",
     "shell.execute_reply": "2025-09-29T15:57:04.382562Z"
    }
   },
   "outputs": [],
   "source": [
    "show_text(df, text_column=\"explanation\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec526d1-e11a-4ac2-a45a-b82f4464d3dd",
   "metadata": {},
   "source": [
    "Checking that indeed we received at most the four options. (It seems that the models rarely says \"I don't know.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57e13f-84ef-4aa6-9661-886eb64f5914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:04.384677Z",
     "iopub.status.busy": "2025-09-29T15:57:04.384445Z",
     "iopub.status.idle": "2025-09-29T15:57:04.389901Z",
     "shell.execute_reply": "2025-09-29T15:57:04.389487Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"hawkishness\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665b015-8489-4f11-8ecf-f6cf941e3762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:04.391393Z",
     "iopub.status.busy": "2025-09-29T15:57:04.391184Z",
     "iopub.status.idle": "2025-09-29T15:57:04.408757Z",
     "shell.execute_reply": "2025-09-29T15:57:04.408317Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"probability\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6dd61-6325-4400-ab22-b3147087767e",
   "metadata": {},
   "source": [
    "To get an intuition from the output of the LLM, we problem the cumulative difference of the dummies `hawkish - dovish`. We see clearly the tightening and accomodating phases of monetary policy:\n",
    "\n",
    "- 1999 - 2001: tightening until the dot-com burst \n",
    "- 2001 - 2004: loosening of monetary policy \n",
    "- 2004 - 2007: tightening\n",
    "- 2007 - 2017: long experiment with loosening, including with Quantitative Easing \n",
    "- 2017 - 2019: Quantitative tightening\n",
    "- 2019 - 2021: economic slowdown (2019) preceding the Covid intervention\n",
    "- 2022 - today: tightening due to high inflation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692ed22-0122-4319-8e4f-ba26d7686be5",
   "metadata": {},
   "source": [
    "The picture below shows  `hawkish-dovish` LLM score with a rolling 1-year (= 8 FOMC meetings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b98163-4bcb-4916-9122-acf42de30c73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:04.410394Z",
     "iopub.status.busy": "2025-09-29T15:57:04.410166Z",
     "iopub.status.idle": "2025-09-29T15:57:04.557881Z",
     "shell.execute_reply": "2025-09-29T15:57:04.557427Z"
    }
   },
   "outputs": [],
   "source": [
    "statements_ = statements.join(\n",
    "    pd.get_dummies(df[\"hawkishness\"], dtype=float).mul(df[\"probability\"], axis=0)\n",
    ").join(df[\"explanation\"])\n",
    "\n",
    "line(\n",
    "    statements_.pipe(lambda x: x[\"hawkish\"].sub(x[\"dovish\"])).rolling(window=8).mean(),\n",
    "    legend=False,\n",
    "    title=\"1-year rolling 'hawkish-dovish' LLM score\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f5510-7649-4988-8d30-04c477f32c84",
   "metadata": {},
   "source": [
    "## Explaining hawkishness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1bb289-eab8-4890-b4f4-7dd6b22a21e2",
   "metadata": {},
   "source": [
    "Beyond the simple validation of the LLM score in the previous section, we can go deeper and identify the words associated with hawkishness (or dovishness) as interpreted by the language model. To do so, we run a regression where the target is the LLM score and the features are `tfidf` values for the main tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafc8f2-51b9-41a8-bf02-5cb0bd4896cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:04.559521Z",
     "iopub.status.busy": "2025-09-29T15:57:04.559300Z",
     "iopub.status.idle": "2025-09-29T15:57:04.570232Z",
     "shell.execute_reply": "2025-09-29T15:57:04.569803Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51539551-7858-486a-94c5-0fa74d73f0ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:04.571728Z",
     "iopub.status.busy": "2025-09-29T15:57:04.571520Z",
     "iopub.status.idle": "2025-09-29T15:57:04.574495Z",
     "shell.execute_reply": "2025-09-29T15:57:04.574067Z"
    }
   },
   "outputs": [],
   "source": [
    "est = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                vocabulary=None,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500,\n",
    "                stop_words=\"english\",\n",
    "                token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"reg\", ElasticNet(alpha=0.0075)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30280d9f-d574-42a4-9e7a-83df328a3261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:04.575922Z",
     "iopub.status.busy": "2025-09-29T15:57:04.575723Z",
     "iopub.status.idle": "2025-09-29T15:57:04.900054Z",
     "shell.execute_reply": "2025-09-29T15:57:04.899547Z"
    }
   },
   "outputs": [],
   "source": [
    "X = statements_[\"text\"]\n",
    "interpret_coefs = {}\n",
    "for c in [\"hawkish\", \"neutral\", \"dovish\"]:\n",
    "    y = statements_[c]\n",
    "    est.fit(X, y)\n",
    "    vocab_ = pd.Series(est.named_steps[\"tfidf\"].vocabulary_).sort_values().index\n",
    "    interpret_coefs[c] = pd.Series(\n",
    "        np.transpose(est.named_steps[\"reg\"].coef_), index=vocab_\n",
    "    )\n",
    "d = {k: v.nlargest(n=10) for k, v in interpret_coefs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983bb26-6aab-4660-999e-ff9090f011aa",
   "metadata": {},
   "source": [
    "Words associated to tightening are on the `hawkish` side (\"raise target\", \"pressures\", \"inflation\", etc). A bit more mixed resuts on the other side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a587d6-1743-4c5b-8521-361f466db049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:04.901878Z",
     "iopub.status.busy": "2025-09-29T15:57:04.901654Z",
     "iopub.status.idle": "2025-09-29T15:57:05.210525Z",
     "shell.execute_reply": "2025-09-29T15:57:05.210017Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 8))\n",
    "fig.subplots_adjust(wspace=0.6)\n",
    "for i, (k, v) in enumerate(d.items()):\n",
    "    bar(v, horizontal=True, ax=ax[i], title=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b75188-a500-44fa-b249-129f8c20be66",
   "metadata": {},
   "source": [
    "Instead of using the actual text, we can use the explanation provided by chatGPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8526f-756f-4e47-9b55-afb35868917d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:05.212358Z",
     "iopub.status.busy": "2025-09-29T15:57:05.212109Z",
     "iopub.status.idle": "2025-09-29T15:57:05.607131Z",
     "shell.execute_reply": "2025-09-29T15:57:05.606613Z"
    }
   },
   "outputs": [],
   "source": [
    "X = statements_[\"explanation\"]\n",
    "interpret_coefs = {}\n",
    "for c in [\"hawkish\", \"neutral\", \"dovish\"]:\n",
    "    y = statements_[c]\n",
    "    est.fit(X, y)\n",
    "    vocab_ = pd.Series(est.named_steps[\"tfidf\"].vocabulary_).sort_values().index\n",
    "    interpret_coefs[c] = pd.Series(\n",
    "        np.transpose(est.named_steps[\"reg\"].coef_), index=vocab_\n",
    "    )\n",
    "d = {k: v.nlargest(n=10) for k, v in interpret_coefs.items()}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 8))\n",
    "fig.subplots_adjust(wspace=0.6)\n",
    "for i, (k, v) in enumerate(d.items()):\n",
    "    bar(v, horizontal=True, ax=ax[i], title=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615affc-f70e-4e8e-8149-24c7fa7c60e8",
   "metadata": {},
   "source": [
    "## Topics in FOMC statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa23cb-8138-4118-bda7-40c2b67449d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:05.608954Z",
     "iopub.status.busy": "2025-09-29T15:57:05.608726Z",
     "iopub.status.idle": "2025-09-29T15:57:05.613233Z",
     "shell.execute_reply": "2025-09-29T15:57:05.612802Z"
    }
   },
   "outputs": [],
   "source": [
    "def content_topics(text):\n",
    "    return f\"\"\"Please assess the importance for each of the following topics in the text below: \n",
    "    inflation, employment, economic growth, financial stability, interest rates and the yield curve, fiscal policy, consumer confidence, and market expectations. \n",
    "    Provide a score between 0 and 1 for each.\n",
    "\n",
    "Text: {text}\"\"\"\n",
    "\n",
    "\n",
    "class format_topics(BaseModel):\n",
    "    inflation: float\n",
    "    employment: float\n",
    "    economic_growth: float\n",
    "    financial_stability: float\n",
    "    interest_rate_and_the_yield_curve: float\n",
    "    fiscal_policy: float\n",
    "    consumer_confidence: float\n",
    "    market_expectations: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b5257-bb1e-4515-8f80-27089de94471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:57:05.614762Z",
     "iopub.status.busy": "2025-09-29T15:57:05.614506Z",
     "iopub.status.idle": "2025-09-29T15:58:09.899884Z",
     "shell.execute_reply": "2025-09-29T15:58:09.899283Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = apply_prompt(\n",
    "    statements,\n",
    "    content_function=content_topics,\n",
    "    response_format=format_topics,\n",
    "    text_column=\"text\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85635089-58b1-46e5-84f0-8f03e7fa02ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:09.902001Z",
     "iopub.status.busy": "2025-09-29T15:58:09.901738Z",
     "iopub.status.idle": "2025-09-29T15:58:10.074255Z",
     "shell.execute_reply": "2025-09-29T15:58:10.073793Z"
    }
   },
   "outputs": [],
   "source": [
    "line(\n",
    "    df2.pipe(lambda x: x.sub(x.rolling(window=12, min_periods=6).mean())),\n",
    "    cumsum=True,\n",
    "    legend_sharpe_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1396f8-93ce-44cb-8981-a61c2986703a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:10.075923Z",
     "iopub.status.busy": "2025-09-29T15:58:10.075705Z",
     "iopub.status.idle": "2025-09-29T15:58:10.078782Z",
     "shell.execute_reply": "2025-09-29T15:58:10.078353Z"
    }
   },
   "outputs": [],
   "source": [
    "est_ = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                vocabulary=None,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500,\n",
    "                stop_words=\"english\",\n",
    "                token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"reg\", ElasticNet(alpha=5 * 1e-4)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951355d3-6269-480b-9f89-935c433d7794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:10.080281Z",
     "iopub.status.busy": "2025-09-29T15:58:10.080080Z",
     "iopub.status.idle": "2025-09-29T15:58:11.731222Z",
     "shell.execute_reply": "2025-09-29T15:58:11.730732Z"
    }
   },
   "outputs": [],
   "source": [
    "statements2_ = statements.join(df2)\n",
    "X = statements2_[\"text\"]\n",
    "interpret_coefs = {}\n",
    "for c in df2.columns:\n",
    "    y = statements2_[c]\n",
    "    est_.fit(X, y)\n",
    "    vocab_ = pd.Series(est_.named_steps[\"tfidf\"].vocabulary_).sort_values().index\n",
    "    interpret_coefs[c] = pd.Series(\n",
    "        np.transpose(est_.named_steps[\"reg\"].coef_), index=vocab_\n",
    "    )\n",
    "d = {k: v.nlargest(n=10) for k, v in interpret_coefs.items()}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(20, 8))\n",
    "ax = ax.ravel()\n",
    "fig.subplots_adjust(wspace=0.9)\n",
    "for i, (k, v) in enumerate(d.items()):\n",
    "    bar(v, horizontal=True, ax=ax[i], title=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09cfe91-836e-43a2-80dd-53559f4adb46",
   "metadata": {},
   "source": [
    "## Predicting returns with FOMC statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f0cee-11d3-4239-b22c-ccafc8ccada2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:11.733009Z",
     "iopub.status.busy": "2025-09-29T15:58:11.732770Z",
     "iopub.status.idle": "2025-09-29T15:58:11.737051Z",
     "shell.execute_reply": "2025-09-29T15:58:11.736621Z"
    }
   },
   "outputs": [],
   "source": [
    "def content_returns(text):\n",
    "    return f\"\"\"Here is a piece of news: {text}. \n",
    "    Do you think this news will increase or decrease the S&P500? Write your answer as:\n",
    "    - impact: increase/decrease/uncertain:\n",
    "    - confidence (0-1):\n",
    "    - magnitude of impact (0-1):\n",
    "    - explanation (less than 25 words)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class format_returns(BaseModel):\n",
    "    sp_impact: Literal[\"increase\", \"decrease\", \"uncertain\"]\n",
    "    confidence: float\n",
    "    magnitude: float\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0c85a-82ba-462b-9114-40a99dd91b51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:11.738574Z",
     "iopub.status.busy": "2025-09-29T15:58:11.738369Z",
     "iopub.status.idle": "2025-09-29T15:58:31.035907Z",
     "shell.execute_reply": "2025-09-29T15:58:31.035307Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = apply_prompt(\n",
    "    statements,\n",
    "    content_function=content_returns,\n",
    "    response_format=format_returns,\n",
    "    text_column=\"text\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fab698-ff33-4860-a902-74721bad2a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:31.037989Z",
     "iopub.status.busy": "2025-09-29T15:58:31.037732Z",
     "iopub.status.idle": "2025-09-29T15:58:31.099798Z",
     "shell.execute_reply": "2025-09-29T15:58:31.099317Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_alpha = (\n",
    "    pd.get_dummies(df3[\"sp_impact\"], dtype=float)\n",
    "    .mul(df3[\"confidence\"] * df3[\"magnitude\"], axis=0)\n",
    "    .pipe(lambda x: x[\"increase\"] - x[\"decrease\"])\n",
    "    .resample(\"B\")\n",
    "    .last()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50ba66-00c4-4b25-8f18-36a5397b3924",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:31.101480Z",
     "iopub.status.busy": "2025-09-29T15:58:31.101247Z",
     "iopub.status.idle": "2025-09-29T15:58:31.223814Z",
     "shell.execute_reply": "2025-09-29T15:58:31.223361Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = (\n",
    "    raw_alpha.pipe(lambda x: x.sub(x.ewm(halflife=6, ignore_na=True).mean()))\n",
    "    .pipe(lambda x: x.div(x.ewm(12, min_periods=6, ignore_na=True).std()))\n",
    "    .ewm(halflife=63)\n",
    "    .mean()\n",
    ")\n",
    "line(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44afc0c4-8e9a-487a-b037-d959e6ca2129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:31.225454Z",
     "iopub.status.busy": "2025-09-29T15:58:31.225231Z",
     "iopub.status.idle": "2025-09-29T15:58:31.436647Z",
     "shell.execute_reply": "2025-09-29T15:58:31.436150Z"
    }
   },
   "outputs": [],
   "source": [
    "from skfin.backtesting import Backtester\n",
    "from skfin.mv_estimators import TimingMeanVariance\n",
    "from skfin.datasets_ import load_kf_returns\n",
    "\n",
    "\n",
    "def transform_y(df):\n",
    "    return df.shift(-2)\n",
    "\n",
    "\n",
    "ret = (\n",
    "    load_kf_returns(filename=\"F-F_Research_Data_Factors_daily\")[\"Daily\"]\n",
    "    .resample(\"B\")\n",
    "    .last()\n",
    ")\n",
    "data = ret.join(alpha.rename(\"alpha\"), how=\"inner\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed54df7-605d-4d9b-bf9f-28e3dbdcb9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:31.438437Z",
     "iopub.status.busy": "2025-09-29T15:58:31.438190Z",
     "iopub.status.idle": "2025-09-29T15:58:32.457169Z",
     "shell.execute_reply": "2025-09-29T15:58:32.456656Z"
    }
   },
   "outputs": [],
   "source": [
    "ret_ = data[\"Mkt-RF\"]\n",
    "y = transform_y(data[\"Mkt-RF\"])\n",
    "m = Backtester(estimator=TimingMeanVariance(), name=\"gpt\", start_date=\"2000-06-30\")\n",
    "pnl_ = m.train(data[\"alpha\"], y, ret_)\n",
    "line(pnl_, cumsum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8883887-c533-45bd-aac8-c441bd88f2d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:32.459003Z",
     "iopub.status.busy": "2025-09-29T15:58:32.458747Z",
     "iopub.status.idle": "2025-09-29T15:58:32.586944Z",
     "shell.execute_reply": "2025-09-29T15:58:32.586485Z"
    }
   },
   "outputs": [],
   "source": [
    "line(\n",
    "    {\"holding\": m.h_, \"tilt\": m.h_.ewm(halflife=252).mean()},\n",
    "    title=\"Holding decomposition\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d071b9a-28bf-47f4-bb5f-f0a89f348374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:32.588583Z",
     "iopub.status.busy": "2025-09-29T15:58:32.588364Z",
     "iopub.status.idle": "2025-09-29T15:58:34.052555Z",
     "shell.execute_reply": "2025-09-29T15:58:34.052062Z"
    }
   },
   "outputs": [],
   "source": [
    "sr = {i: m.h_.shift(1 + i).mul(ret_).pipe(sharpe_ratio) for i in range(-10, 12)}\n",
    "bar(sr, baseline=0, sort=False, title=\"Lead-lag sharpe ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c747c4e-8713-4848-b10d-9d3a7b4e63a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:34.054356Z",
     "iopub.status.busy": "2025-09-29T15:58:34.054096Z",
     "iopub.status.idle": "2025-09-29T15:58:34.366115Z",
     "shell.execute_reply": "2025-09-29T15:58:34.365649Z"
    }
   },
   "outputs": [],
   "source": [
    "line(\n",
    "    {\n",
    "        \"ALL\": pnl_,\n",
    "        \"tilt\": m.h_.ewm(halflife=12).mean().shift(1).mul(ret_).dropna(),\n",
    "        \"timing\": m.h_.sub(m.h_.ewm(halflife=12).mean()).shift(1).mul(ret_).dropna(),\n",
    "    },\n",
    "    cumsum=True,\n",
    "    title=\"Tilt/timing decomposition\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0f51c-be88-4bd7-947e-34525217cdc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:58:34.367816Z",
     "iopub.status.busy": "2025-09-29T15:58:34.367590Z",
     "iopub.status.idle": "2025-09-29T15:58:34.684619Z",
     "shell.execute_reply": "2025-09-29T15:58:34.684122Z"
    }
   },
   "outputs": [],
   "source": [
    "line(\n",
    "    {\n",
    "        \"ALL\": pnl_,\n",
    "        \"long\": m.h_.clip(lower=0).shift(1).mul(ret_).dropna(),\n",
    "        \"short\": m.h_.clip(upper=0).shift(1).mul(ret_).dropna(),\n",
    "    },\n",
    "    cumsum=True,\n",
    "    title=\"Long/short decomposition\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skfin 2024",
   "language": "python",
   "name": "skfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
