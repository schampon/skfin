{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d4ff7b-8075-44a3-9e0d-6b1467965bc0",
   "metadata": {},
   "source": [
    "# Using Large Language Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a88c8-54db-4182-a747-99bd7b60725f",
   "metadata": {},
   "source": [
    "In this notebook, we illustrate how to use `Large Language Models` (LLM), in particular models by Openai to extract information for each document of a corpus. The corpus that we will use is the collection of statements by the US Federal Reserve since 1999. \n",
    "\n",
    "In particular, given that Openai only proposes commercial LLMs, you need to have created an account and been provided a key. In the cell below you can paste the key: `api_key=sk-...`. Alternatively (as done below), you can put in a file call \"key.\" and call the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd1428-db8a-4a24-ac6d-6ad5bc0f316e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:13.970288Z",
     "iopub.status.busy": "2024-11-07T17:06:13.969999Z",
     "iopub.status.idle": "2024-11-07T17:06:15.460333Z",
     "shell.execute_reply": "2024-11-07T17:06:15.459755Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "with open(\"/home/schamponn/dev/projects/llm/key\", \"r\") as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "import os\n",
    "\n",
    "del os.environ[\"http_proxy\"]\n",
    "del os.environ[\"https_proxy\"]\n",
    "\n",
    "import logging\n",
    "\n",
    "httpx_logger = logging.getLogger(\"httpx\")\n",
    "httpx_logger.setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from skfin.datasets import load_fomc_statements\n",
    "from skfin.text import show_text\n",
    "from skfin.plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db6266-b340-48a2-b471-536354a70f09",
   "metadata": {},
   "source": [
    "## LLM wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3150238-6ca8-4349-a79f-fa272224dcf7",
   "metadata": {},
   "source": [
    "The following functions in `llm.py` allows to access language models by `openai`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0700b-5e83-4c52-a43b-0ae331e7df4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:15.463811Z",
     "iopub.status.busy": "2024-11-07T17:06:15.463356Z",
     "iopub.status.idle": "2024-11-07T17:06:15.472663Z",
     "shell.execute_reply": "2024-11-07T17:06:15.472225Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile ../skfin/llm.py\n",
    "import ssl\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "@retry(wait=wait_exponential(multiplier=1, min=6, max=10), stop=stop_after_attempt(5))\n",
    "def apply_prompt_single_text(\n",
    "    text,\n",
    "    content_function,\n",
    "    response_format,\n",
    "    nested_response=False,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_new_tokens=2048,\n",
    "    temperature=0,\n",
    "    api_key=None,\n",
    "):\n",
    "    \"\"\"Apply a specified prompt to a single text input using an OpenAI GPT model, and format the response.\n",
    "\n",
    "    This function retries on failure with an exponential backoff strategy up to 5 attempts.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be processed by the prompt.\n",
    "        content_func (callable): A function that takes a single string argument (text) and returns a formatted string.\n",
    "        response_format (ResponseFormat): An instance specifying the desired format of the response.\n",
    "        model (str, optional): The identifier for the model to use. Defaults to \"gpt-4o-mini\".\n",
    "        max_new_tokens (int, optional): The maximum number of new tokens to generate in the response. Defaults to 2048.\n",
    "        temperature (float, optional): Sampling temperature to use for the response generation. A value closer to 0 makes the output more deterministic. Defaults to 0.\n",
    "        api_key (str, optional): OpenAi key. Defaults to None, in which case, it loaded in the local folder.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys correspond to the properties specified in the response format's JSON schema,\n",
    "              and values are the parsed responses from the model. Returns 'NA' if no output is generated.\n",
    "\n",
    "    Raises:\n",
    "        openai.error.OpenAIError: If fails to get a response after the specified retry attempts.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Assume the role of a financial analyst.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_function(text),\n",
    "        },\n",
    "    ]\n",
    "    if api_key is None:\n",
    "        with open(\"key\", \"r\") as f:\n",
    "            api_key = f.read()\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format=response_format,\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=0,\n",
    "        seed=42,\n",
    "        logprobs=True,\n",
    "        top_logprobs=10,\n",
    "        top_p=1,\n",
    "    )\n",
    "    message = completion.choices[0].message\n",
    "    if message.parsed:\n",
    "        keys = list(response_format.model_json_schema()[\"properties\"].keys())\n",
    "        output = {k: message.parsed.__dict__[k] for k in keys}\n",
    "        if nested_response:\n",
    "            assert len(output) == 1\n",
    "            output = output[list(output.keys())[0]]\n",
    "            assert isinstance(output, list)\n",
    "            k, v = output[0].model_json_schema()[\"properties\"].keys()\n",
    "            output = pd.Series({c.__dict__[k]: c.__dict__[v] for c in output})\n",
    "    else:\n",
    "        print(\"No output\")\n",
    "        output = \"NA\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def apply_prompt(\n",
    "    dataframe,\n",
    "    content_function,\n",
    "    response_format,\n",
    "    nested_response=False,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    text_column=\"text\",\n",
    "    max_workers=25,\n",
    "    sequential=False,\n",
    "    return_dataframe=True,\n",
    "    api_key=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply a specified prompt to a DataFrame containing text data using an OpenAI GPT model.\n",
    "\n",
    "    This function can process the text data either sequentially or concurrently using a thread pool.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing the text data to be processed.\n",
    "        content_function (callable): A function that takes a single string argument (text) and returns a formatted string.\n",
    "        response_format (ResponseFormat): An instance specifying the desired format of the response.\n",
    "        text_columns (str, optional): The column name in the DataFrame that contains the text data. Defaults to 'text'.\n",
    "        max_workers (int, optional): The maximum number of worker threads to use for concurrent execution. Defaults to 25.\n",
    "        sequential (bool, optional): If True, process the text data sequentially. If False, process the data concurrently. Defaults to False.\n",
    "        return_dataframe (bool, optional): If True, return the results as a new DataFrame. If False, return the results as a list. Defaults to True.\n",
    "        api_key (str, optional): OpenAi key. Defaults to None, in which case, it loaded in the local folder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or list: The processed output. A DataFrame if `return_dataframe` is True, otherwise a list.\n",
    "    \"\"\"\n",
    "\n",
    "    texts = dataframe[text_column].values\n",
    "\n",
    "    if sequential:\n",
    "        results = [\n",
    "            apply_prompt_single_text(\n",
    "                text,\n",
    "                content_function=content_function,\n",
    "                response_format=response_format,\n",
    "                nested_response=nested_response,\n",
    "                model=model,\n",
    "                api_key=api_key,\n",
    "            )\n",
    "            for text in texts\n",
    "        ]\n",
    "    else:\n",
    "        apply_prompt_single_text_partial = partial(\n",
    "            apply_prompt_single_text,\n",
    "            content_function=content_function,\n",
    "            response_format=response_format,\n",
    "            nested_response=nested_response,\n",
    "            model=model,\n",
    "            api_key=api_key,\n",
    "        )\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = list(executor.map(apply_prompt_single_text_partial, texts))\n",
    "\n",
    "    if return_dataframe:\n",
    "        return pd.DataFrame(results, index=dataframe.index)\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2c861-d0fc-4370-a34d-d14d73de5ad0",
   "metadata": {},
   "source": [
    "In the function above (and the analysis below), the baseline model is `gpt-4o-mini`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cfb529-ce52-4a21-bdc0-7a6963a9637c",
   "metadata": {},
   "source": [
    "## Measuring hawkishness in FOMC statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9551a97-0b96-4f1a-9043-8f25a9002b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:15.475146Z",
     "iopub.status.busy": "2024-11-07T17:06:15.474540Z",
     "iopub.status.idle": "2024-11-07T17:06:15.570244Z",
     "shell.execute_reply": "2024-11-07T17:06:15.569748Z"
    }
   },
   "outputs": [],
   "source": [
    "statements = load_fomc_statements(force_reload=False, cache_dir=\"../nbs/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99464686-5644-46ec-883f-da255b43b9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:15.572572Z",
     "iopub.status.busy": "2024-11-07T17:06:15.572219Z",
     "iopub.status.idle": "2024-11-07T17:06:15.577726Z",
     "shell.execute_reply": "2024-11-07T17:06:15.577313Z"
    }
   },
   "outputs": [],
   "source": [
    "text = statements[\"text\"].iloc[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad770b2-6a37-4d08-b31e-ea59b8c4a71b",
   "metadata": {},
   "source": [
    "With the following prompt, we ask the model to qualify Fed statements in terms of hawkishness/dovishness. We also insist on receiving a single-word answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf4e6c-b02e-4434-b8db-864f3def3f3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:15.579912Z",
     "iopub.status.busy": "2024-11-07T17:06:15.579520Z",
     "iopub.status.idle": "2024-11-07T17:06:16.513202Z",
     "shell.execute_reply": "2024-11-07T17:06:16.512493Z"
    }
   },
   "outputs": [],
   "source": [
    "from skfin.llm import apply_prompt\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def content_hawkishness(text):\n",
    "    return f\"\"\"Act as a financial analyst. What is the monetary policy hawkishness of this text?\n",
    "    Please choose an answer from hawkish, dovish, neutral or unknown and provide a probability and a short explanation. \n",
    "\n",
    "Text: {text}\"\"\"\n",
    "\n",
    "\n",
    "class format_hawkishness(BaseModel):\n",
    "    hawkishness: Literal[\"hawkish\", \"neutral\", \"dovish\", \"unknown\"]\n",
    "    probability: float\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d049d-6d66-4452-98a8-dd25ea20e87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:16.516465Z",
     "iopub.status.busy": "2024-11-07T17:06:16.516083Z",
     "iopub.status.idle": "2024-11-07T17:06:35.839403Z",
     "shell.execute_reply": "2024-11-07T17:06:35.838843Z"
    }
   },
   "outputs": [],
   "source": [
    "df = apply_prompt(\n",
    "    statements,\n",
    "    content_function=content_hawkishness,\n",
    "    response_format=format_hawkishness,\n",
    "    text_column=\"text\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cbdc5f-73c6-434d-afdc-a85c265d4f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:35.842392Z",
     "iopub.status.busy": "2024-11-07T17:06:35.842052Z",
     "iopub.status.idle": "2024-11-07T17:06:35.849567Z",
     "shell.execute_reply": "2024-11-07T17:06:35.849137Z"
    }
   },
   "outputs": [],
   "source": [
    "show_text(df, text_column=\"explanation\", n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b455833-1ea3-49ee-9692-a8ea380cffa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T18:03:14.609833Z",
     "iopub.status.busy": "2024-11-06T18:03:14.609482Z",
     "iopub.status.idle": "2024-11-06T18:03:14.618345Z",
     "shell.execute_reply": "2024-11-06T18:03:14.617864Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ec526d1-e11a-4ac2-a45a-b82f4464d3dd",
   "metadata": {},
   "source": [
    "Checking that indeed we received at most the four options. (It seems that the models rarely says \"I don't know.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57e13f-84ef-4aa6-9661-886eb64f5914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:35.851984Z",
     "iopub.status.busy": "2024-11-07T17:06:35.851466Z",
     "iopub.status.idle": "2024-11-07T17:06:35.859355Z",
     "shell.execute_reply": "2024-11-07T17:06:35.858964Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"hawkishness\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665b015-8489-4f11-8ecf-f6cf941e3762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:35.861382Z",
     "iopub.status.busy": "2024-11-07T17:06:35.860994Z",
     "iopub.status.idle": "2024-11-07T17:06:35.871234Z",
     "shell.execute_reply": "2024-11-07T17:06:35.870825Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"probability\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6dd61-6325-4400-ab22-b3147087767e",
   "metadata": {},
   "source": [
    "To get an intuition from the output of the LLM, we problem the cumulative difference of the dummies `hawkish - dovish`. We see clearly the tightening and accomodating phases of monetary policy:\n",
    "\n",
    "- 1999 - 2001: tightening until the dot-com burst \n",
    "- 2001 - 2004: loosening of monetary policy \n",
    "- 2004 - 2007: tightening\n",
    "- 2007 - 2017: long experiment with loosening, including with Quantitative Easing \n",
    "- 2017 - 2019: Quantitative tightening\n",
    "- 2019 - 2021: economic slowdown (2019) preceding the Covid intervention\n",
    "- 2022 - today: tightening due to high inflation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692ed22-0122-4319-8e4f-ba26d7686be5",
   "metadata": {},
   "source": [
    "The picture below shows  `hawkish-dovish` LLM score with a rolling 1-year (= 8 FOMC meetings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b98163-4bcb-4916-9122-acf42de30c73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:35.873360Z",
     "iopub.status.busy": "2024-11-07T17:06:35.872974Z",
     "iopub.status.idle": "2024-11-07T17:06:36.009072Z",
     "shell.execute_reply": "2024-11-07T17:06:36.008630Z"
    }
   },
   "outputs": [],
   "source": [
    "statements_ = statements.join(\n",
    "    pd.get_dummies(df[\"hawkishness\"], dtype=float).mul(df[\"probability\"], axis=0)\n",
    ").join(df[\"explanation\"])\n",
    "\n",
    "line(\n",
    "    statements_.pipe(lambda x: x[\"hawkish\"].sub(x[\"dovish\"])).rolling(window=8).mean(),\n",
    "    legend=False,\n",
    "    title=\"1-year rolling 'hawkish-dovish' LLM score\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f5510-7649-4988-8d30-04c477f32c84",
   "metadata": {},
   "source": [
    "## Explaining hawkishness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1bb289-eab8-4890-b4f4-7dd6b22a21e2",
   "metadata": {},
   "source": [
    "Beyond the simple validation of the LLM score in the previous section, we can go deeper and identify the words associated with hawkishness (or dovishness) as interpreted by the language model. To do so, we run a regression where the target is the LLM score and the features are `tfidf` values for the main tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafc8f2-51b9-41a8-bf02-5cb0bd4896cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:36.011402Z",
     "iopub.status.busy": "2024-11-07T17:06:36.011055Z",
     "iopub.status.idle": "2024-11-07T17:06:36.019810Z",
     "shell.execute_reply": "2024-11-07T17:06:36.019389Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51539551-7858-486a-94c5-0fa74d73f0ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:36.022016Z",
     "iopub.status.busy": "2024-11-07T17:06:36.021513Z",
     "iopub.status.idle": "2024-11-07T17:06:36.024710Z",
     "shell.execute_reply": "2024-11-07T17:06:36.024305Z"
    }
   },
   "outputs": [],
   "source": [
    "est = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                vocabulary=None,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500,\n",
    "                stop_words=\"english\",\n",
    "                token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"reg\", ElasticNet(alpha=0.0075)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30280d9f-d574-42a4-9e7a-83df328a3261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:36.026582Z",
     "iopub.status.busy": "2024-11-07T17:06:36.026325Z",
     "iopub.status.idle": "2024-11-07T17:06:36.324431Z",
     "shell.execute_reply": "2024-11-07T17:06:36.323886Z"
    }
   },
   "outputs": [],
   "source": [
    "X = statements_[\"text\"]\n",
    "interpret_coefs = {}\n",
    "for c in [\"hawkish\", \"neutral\", \"dovish\"]:\n",
    "    y = statements_[c]\n",
    "    est.fit(X, y)\n",
    "    vocab_ = pd.Series(est.named_steps[\"tfidf\"].vocabulary_).sort_values().index\n",
    "    interpret_coefs[c] = pd.Series(\n",
    "        np.transpose(est.named_steps[\"reg\"].coef_), index=vocab_\n",
    "    )\n",
    "d = {k: v.nlargest(n=10) for k, v in interpret_coefs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983bb26-6aab-4660-999e-ff9090f011aa",
   "metadata": {},
   "source": [
    "Words associated to tightening are on the `hawkish` side (\"raise target\", \"pressures\", \"inflation\", etc). A bit more mixed resuts on the other side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a587d6-1743-4c5b-8521-361f466db049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:36.327364Z",
     "iopub.status.busy": "2024-11-07T17:06:36.326869Z",
     "iopub.status.idle": "2024-11-07T17:06:36.731453Z",
     "shell.execute_reply": "2024-11-07T17:06:36.730925Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 8))\n",
    "fig.subplots_adjust(wspace=0.6)\n",
    "for i, (k, v) in enumerate(d.items()):\n",
    "    bar(v, horizontal=True, ax=ax[i], title=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b75188-a500-44fa-b249-129f8c20be66",
   "metadata": {},
   "source": [
    "Instead of using the actual text, we can use the explanation provided by chatGPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8526f-756f-4e47-9b55-afb35868917d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:36.734017Z",
     "iopub.status.busy": "2024-11-07T17:06:36.733706Z",
     "iopub.status.idle": "2024-11-07T17:06:37.108461Z",
     "shell.execute_reply": "2024-11-07T17:06:37.107926Z"
    }
   },
   "outputs": [],
   "source": [
    "X = statements_[\"explanation\"]\n",
    "interpret_coefs = {}\n",
    "for c in [\"hawkish\", \"neutral\", \"dovish\"]:\n",
    "    y = statements_[c]\n",
    "    est.fit(X, y)\n",
    "    vocab_ = pd.Series(est.named_steps[\"tfidf\"].vocabulary_).sort_values().index\n",
    "    interpret_coefs[c] = pd.Series(\n",
    "        np.transpose(est.named_steps[\"reg\"].coef_), index=vocab_\n",
    "    )\n",
    "d = {k: v.nlargest(n=10) for k, v in interpret_coefs.items()}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 8))\n",
    "fig.subplots_adjust(wspace=0.6)\n",
    "for i, (k, v) in enumerate(d.items()):\n",
    "    bar(v, horizontal=True, ax=ax[i], title=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615affc-f70e-4e8e-8149-24c7fa7c60e8",
   "metadata": {},
   "source": [
    "## Topics in FOMC statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa23cb-8138-4118-bda7-40c2b67449d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:37.111167Z",
     "iopub.status.busy": "2024-11-07T17:06:37.110844Z",
     "iopub.status.idle": "2024-11-07T17:06:37.115791Z",
     "shell.execute_reply": "2024-11-07T17:06:37.115341Z"
    }
   },
   "outputs": [],
   "source": [
    "def content_topics(text):\n",
    "    return f\"\"\"Please assess the importance for each of the following topics in the text below: \n",
    "    inflation, employment, economic growth, financial stability, interest rates and the yield curve, fiscal policy, consumer confidence, and market expectations. \n",
    "    Provide a score between 0 and 1 for each.\n",
    "\n",
    "Text: {text}\"\"\"\n",
    "\n",
    "\n",
    "class format_topics(BaseModel):\n",
    "    inflation: float\n",
    "    employment: float\n",
    "    economic_growth: float\n",
    "    financial_stability: float\n",
    "    interest_rate_and_the_yield_curve: float\n",
    "    fiscal_policy: float \n",
    "    consumer_confidence: float \n",
    "    market_expectations: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b5257-bb1e-4515-8f80-27089de94471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:37.118014Z",
     "iopub.status.busy": "2024-11-07T17:06:37.117558Z",
     "iopub.status.idle": "2024-11-07T17:06:52.303084Z",
     "shell.execute_reply": "2024-11-07T17:06:52.302521Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = apply_prompt(\n",
    "    statements,\n",
    "    content_function=content_topics,\n",
    "    response_format=format_topics,\n",
    "    text_column=\"text\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85635089-58b1-46e5-84f0-8f03e7fa02ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:52.305950Z",
     "iopub.status.busy": "2024-11-07T17:06:52.305632Z",
     "iopub.status.idle": "2024-11-07T17:06:52.469307Z",
     "shell.execute_reply": "2024-11-07T17:06:52.468825Z"
    }
   },
   "outputs": [],
   "source": [
    "line(df2.pipe(lambda x: x.sub(x.rolling(window=12, min_periods=6).mean())), cumsum=True, legend_sharpe_ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1396f8-93ce-44cb-8981-a61c2986703a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:52.472464Z",
     "iopub.status.busy": "2024-11-07T17:06:52.471946Z",
     "iopub.status.idle": "2024-11-07T17:06:52.475383Z",
     "shell.execute_reply": "2024-11-07T17:06:52.474886Z"
    }
   },
   "outputs": [],
   "source": [
    "est_ = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"tfidf\",\n",
    "            TfidfVectorizer(\n",
    "                vocabulary=None,\n",
    "                ngram_range=(1, 3),\n",
    "                max_features=500,\n",
    "                stop_words=\"english\",\n",
    "                token_pattern=r\"\\b[a-zA-Z]{3,}\\b\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"reg\", ElasticNet(alpha=5*1e-4)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951355d3-6269-480b-9f89-935c433d7794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:52.477546Z",
     "iopub.status.busy": "2024-11-07T17:06:52.477122Z",
     "iopub.status.idle": "2024-11-07T17:06:54.213996Z",
     "shell.execute_reply": "2024-11-07T17:06:54.213469Z"
    }
   },
   "outputs": [],
   "source": [
    "statements2_ = statements.join(df2)\n",
    "X = statements2_[\"text\"]\n",
    "interpret_coefs = {}\n",
    "for c in df2.columns:\n",
    "    y = statements2_[c]\n",
    "    est_.fit(X, y)\n",
    "    vocab_ = pd.Series(est_.named_steps[\"tfidf\"].vocabulary_).sort_values().index\n",
    "    interpret_coefs[c] = pd.Series(\n",
    "        np.transpose(est_.named_steps[\"reg\"].coef_), index=vocab_\n",
    "    )\n",
    "d = {k: v.nlargest(n=10) for k, v in interpret_coefs.items()}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(20, 8))\n",
    "ax = ax.ravel()\n",
    "fig.subplots_adjust(wspace=0.9)\n",
    "for i, (k, v) in enumerate(d.items()):\n",
    "    bar(v, horizontal=True, ax=ax[i], title=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09cfe91-836e-43a2-80dd-53559f4adb46",
   "metadata": {},
   "source": [
    "## Predicting returns with FOMC statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f0cee-11d3-4239-b22c-ccafc8ccada2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:54.216933Z",
     "iopub.status.busy": "2024-11-07T17:06:54.216512Z",
     "iopub.status.idle": "2024-11-07T17:06:54.221215Z",
     "shell.execute_reply": "2024-11-07T17:06:54.220794Z"
    }
   },
   "outputs": [],
   "source": [
    "def content_returns(text):\n",
    "    return f\"\"\"Here is a piece of news: {text}. \n",
    "    Do you think this news will increase or decrease the S&P500? Write your answer as:\n",
    "    - impact: increase/decrease/uncertain:\n",
    "    - confidence (0-1):\n",
    "    - magnitude of impact (0-1):\n",
    "    - explanation (less than 25 words)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class format_returns(BaseModel):\n",
    "    sp_impact: Literal[\"increase\", \"decrease\", \"uncertain\"]\n",
    "    confidence: float\n",
    "    magnitude: float\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0c85a-82ba-462b-9114-40a99dd91b51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:06:54.223335Z",
     "iopub.status.busy": "2024-11-07T17:06:54.222921Z",
     "iopub.status.idle": "2024-11-07T17:07:05.652877Z",
     "shell.execute_reply": "2024-11-07T17:07:05.652337Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = apply_prompt(\n",
    "    statements,\n",
    "    content_function=content_returns,\n",
    "    response_format=format_returns,\n",
    "    text_column=\"text\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fab698-ff33-4860-a902-74721bad2a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:07:05.655866Z",
     "iopub.status.busy": "2024-11-07T17:07:05.655558Z",
     "iopub.status.idle": "2024-11-07T17:07:05.711778Z",
     "shell.execute_reply": "2024-11-07T17:07:05.711190Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_alpha = pd.get_dummies(df3[\"sp_impact\"], dtype=float).mul(df3[\"confidence\"] * df3[\"magnitude\"], axis=0).pipe(lambda x: x['increase'] - x['decrease'])\\\n",
    "                .resample('B').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50ba66-00c4-4b25-8f18-36a5397b3924",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:07:05.714743Z",
     "iopub.status.busy": "2024-11-07T17:07:05.714393Z",
     "iopub.status.idle": "2024-11-07T17:07:05.842823Z",
     "shell.execute_reply": "2024-11-07T17:07:05.842343Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = raw_alpha.pipe(lambda x: x.sub(x.ewm(halflife=6, ignore_na=True).mean()))\\\n",
    "                  .pipe(lambda x: x.div(x.ewm(12, min_periods=6, ignore_na=True).std()))\\\n",
    "                  .ewm(halflife=63).mean()\n",
    "line(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44afc0c4-8e9a-487a-b037-d959e6ca2129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:07:05.844952Z",
     "iopub.status.busy": "2024-11-07T17:07:05.844676Z",
     "iopub.status.idle": "2024-11-07T17:07:06.073360Z",
     "shell.execute_reply": "2024-11-07T17:07:06.072868Z"
    }
   },
   "outputs": [],
   "source": [
    "from skfin.backtesting import Backtester\n",
    "from skfin.mv_estimators import TimingMeanVariance\n",
    "from skfin.datasets import  load_kf_returns\n",
    "\n",
    "def transform_y(df):\n",
    "    return df.shift(-2)\n",
    "\n",
    "ret =load_kf_returns(filename=\"F-F_Research_Data_Factors_daily\")['Daily'].resample('B').last()\n",
    "data = ret.join(alpha.rename('alpha'), how='inner').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed54df7-605d-4d9b-bf9f-28e3dbdcb9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:07:06.075747Z",
     "iopub.status.busy": "2024-11-07T17:07:06.075386Z",
     "iopub.status.idle": "2024-11-07T17:07:06.843350Z",
     "shell.execute_reply": "2024-11-07T17:07:06.842884Z"
    }
   },
   "outputs": [],
   "source": [
    "ret_ = data['Mkt-RF']\n",
    "y = transform_y(data['Mkt-RF'])\n",
    "m = Backtester(estimator=TimingMeanVariance(), name=\"gpt\", start_date='2000-06-30')\n",
    "pnl_ = m.train(data['alpha'], y, ret_)\n",
    "line(pnl_, cumsum=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8883887-c533-45bd-aac8-c441bd88f2d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:07:06.845695Z",
     "iopub.status.busy": "2024-11-07T17:07:06.845325Z",
     "iopub.status.idle": "2024-11-07T17:07:06.972986Z",
     "shell.execute_reply": "2024-11-07T17:07:06.972531Z"
    }
   },
   "outputs": [],
   "source": [
    "line({\"holding\": m.h_, \"tilt\": m.h_.ewm(halflife=252).mean()}, title='Holding decomposition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d071b9a-28bf-47f4-bb5f-f0a89f348374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:07:06.975179Z",
     "iopub.status.busy": "2024-11-07T17:07:06.974899Z",
     "iopub.status.idle": "2024-11-07T17:07:08.278849Z",
     "shell.execute_reply": "2024-11-07T17:07:08.278361Z"
    }
   },
   "outputs": [],
   "source": [
    "sr = {i: m.h_.shift(1 + i).mul(ret_).pipe(sharpe_ratio) for i in range(-10, 12)}\n",
    "bar(sr, baseline=0, sort=False, title=\"Lead-lag sharpe ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c747c4e-8713-4848-b10d-9d3a7b4e63a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:07:08.281410Z",
     "iopub.status.busy": "2024-11-07T17:07:08.280851Z",
     "iopub.status.idle": "2024-11-07T17:07:08.574867Z",
     "shell.execute_reply": "2024-11-07T17:07:08.574402Z"
    }
   },
   "outputs": [],
   "source": [
    "line(\n",
    "    {\n",
    "        \"ALL\": pnl_,\n",
    "        \"tilt\": m.h_.ewm(halflife=12).mean().shift(1).mul(ret_).dropna(),\n",
    "        \"timing\": m.h_.sub(m.h_.ewm(halflife=12).mean()).shift(1).mul(ret_).dropna(),\n",
    "    },\n",
    "    cumsum=True, title='Tilt/timing decomposition'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0f51c-be88-4bd7-947e-34525217cdc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:07:08.577125Z",
     "iopub.status.busy": "2024-11-07T17:07:08.576834Z",
     "iopub.status.idle": "2024-11-07T17:07:08.873962Z",
     "shell.execute_reply": "2024-11-07T17:07:08.873513Z"
    }
   },
   "outputs": [],
   "source": [
    "line(\n",
    "    {\n",
    "        \"ALL\": pnl_,\n",
    "        \"long\": m.h_.clip(lower=0).shift(1).mul(ret_).dropna(),\n",
    "        \"short\": m.h_.clip(upper=0).shift(1).mul(ret_).dropna(),\n",
    "    },\n",
    "    cumsum=True, title='Long/short decomposition'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skfin 2024",
   "language": "python",
   "name": "skfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
