{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What hedge funds do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material is an introduction to using machine-learning for portfolio management and trading. Given the centrality of programming in hedge funds today, the concepts are exposed using only `jupyter` notebooks in `python`. Moreover, we leverage the `scikit-learn` package (also known as `sklearn`) to illustrate how machine-learning is used in practice in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image(\"images/ml_triangle.png\", width=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in how quantitative hedge funds operate in practice. Today, quantitative hedge funds are essentially *consumers* of data -- they ingest all sorts of datasets and extract information used to buy or sell systematically securities. Researchers and portfolio managers are deeply involved in the process of `data ingestion` and `information extraction`, but they do not directly decide which securities are bought or sold -- instead algorithms do.\n",
    "\n",
    "Because these processes of `data ingestion` and `information extraction` are so central to quantitative hedge fund operations, they have become `software companies` -- a lot of the intellectual property (IP) of hedge funds is embedded in the code they write. And in that sense, hedge funds are not so different from other data-science based technology companies. (And in fact the hiring has become very similar, with a lot of interest in profiles out of Computer Science, Machine-Learning, Data engeeniring, Statistics, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More precisely, as [Francois Chollet (04/03/2019)](https://twitter.com/fchollet/status/1113476428249464833) points out, the issue is how to process (ie. test empirically) these new ideas -- as fast as possible to gain a competitive edge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "display(Image(\"images/kaggle_iterations2.png\", width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across notebooks, we cover three main empirical use-cases: timing industry returns; stock returns; and the market returns. For each use-case, there is a set of toy-datasets (described in the `Data` section).\n",
    "\n",
    "Moreover: \n",
    "\n",
    "- for the industry return backtest, we illustrate several main concepts: learning (with linear models, boosted trees, and MLP neural nets); risk; factor exposures; transaction costs; ensemble; \n",
    "- for the stock return backtest, we illustrate mean-reversion, trading around corporate events and sentiment (in earning calls); \n",
    "- for the market timing backtest, we also illustrate traing around macroeconomic events, and extracting sentiment in the statements of the Federal Open Market Committee (FOMC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image(\"images/book_overview.png\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quant workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below shows the typical data workflow of a quantitative fund: \n",
    "\n",
    "- raw data is acquired generally by a Data team and possibly transformed into usable features. \n",
    "- from these features, predictors of asset returns are constructed \n",
    "- given a single predictor (or a set of many predictors), porfolios are constructed: these portfolios represent the ideal positions of a fund given the asset forecasts, but also risk forecasts (and possibly, transaction cost forecasts). \n",
    "- when these ideal positions change from one day to the next (because the underlying data has been updated), the difference in positions initiate trades that are then executed on asset markets or with brokers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image(\"images/quant_workflow.png\", width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLOps (machine learning operations) represents a set of practices for the deployment of ML models in production. For quant hedge funds, there are two main concepts that we describe here:  \n",
    "\n",
    "- pipelines \n",
    "\n",
    "- backtests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline: \n",
    "\n",
    "> A machine-learning pipeline is an end-to-end description of the automated flow of data from raw inputs to a desired output. Each step represents a transformation of the data, possibly with a fitted model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below illustrates a pipeline for a quant fund. The end point (to the right of the diagram) are the positions or `holdings` in a set of traded securities -- and combined with the returns on these securities, the pnl of a given strategy can be computed. The entry point (to the left of the diagram) are `features`. A set of `transformations` (pre-determined in the `pipeline`) are applied to these `features` to produce the desired `holdings`. Some `transformations` in the `pipeline` are \"fixed\" while others depend on `fitted models` (e.g. a ML predictor of returns or a risk model). \n",
    "\n",
    "In the diagram, we emphasize the timing of these different objects: \n",
    "\n",
    "- for a pnl at time $t$, the `features` and `target` include only information up to $t-1$ so that the holdings are known in $t-1$ and can accrue returns over the period $t$. \n",
    "\n",
    "The following equation summarizes this point: \n",
    "\n",
    "$$ pnl_t = holdings_{t-1} \\times returns_t. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image(\"images/ml4pmt_pipeline3.png\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebooks and notes are largely based on  `scikit-learn`. `scikit-learn` is an extremely powerful (and widely used) package for machine-learning. In particular, it provides a \"grammar\" for pipelines  where each transformation or estimator class has the `fit`/`transform`/`predict` functions with arguments as (`X`, `y`) where `X` represents the  features and `y`, targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look-ahead bias occurs when data dated at $t$ includes information only available after $t$; in contrast, point-in-time data ensures that data dated at t is based on only information up to date t. A backtest is a method to simulate a strategy using point-in-time historical data and evaluate its profitability. \n",
    "\n",
    "In order to illustrate how to use pipelines à la `scikit-learn` for quantitative portfolio management, we introduce two object: \n",
    "\n",
    "- a `Mean-variance` estimator that computes positions from a predictor\n",
    "- a `Backtester` class that fits a `scikit-learn` estimator over rolling windows (as defined by the `TimeSeriesSplit` class). \n",
    "\n",
    "The `Backtester` class runs the rolling window simulation so that only information up to date $t-1$ is used to determined the holdings at that date. The following graph shows an example of a learning pipeline (with a `StandardScaler` and `Ridge` steps before compute the positions with the `Mean-variance` estimator). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "display(Image(\"images/ml_pipeline_example.png\", width=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the classes and functions used across notebooks are stored in `.py` files using the `jupyter magic`: `%%writefile`. And these files are structured as a repository on `github` that can be cloned from the command line (once in the correct directory):  \n",
    "> ```git clone https://github.com/schampon/skfin.git && cd skfin```\n",
    "\n",
    "The following script helps create an environment with the proper packages: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../create_env.sh\n",
    "conda create python=3.9 --name  skfin -c https://conda.anaconda.org/conda-forge/ -y\n",
    "conda activate skfin\n",
    "\n",
    "pip install -r requirements.txt\n",
    "pip install -e . \n",
    "python -m ipykernel install --user --name skfin --display-name \"Python (skfin)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, this is done from the command line with: \n",
    "    \n",
    "> ``` ./create_env.sh ```"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "[conda-skfin]",
   "language": "python",
   "name": "conda-env-conda-skfin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
