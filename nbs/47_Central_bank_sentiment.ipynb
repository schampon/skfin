{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment in FOMC statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from skfin.plot import bar, line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment in FOMC statements: Loughran-McDonalds dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we measure sentiment with the Loughran-McDonalds sentiment dictionary in two ways: \n",
    "- sentiment = (#positive - #negative)/(#positive + #negative)\n",
    "- sentiment = (#positive - #negative)/(#words)\n",
    "\n",
    "In the first case, short documents (with few or no sentiment words) might lead to biased estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from skfin.datasets import load_fomc_statements, load_loughran_mcdonald_dictionary\n",
    "from skfin.text import coefs_plot\n",
    "from skfin.text import show_text\n",
    "from skfin.plot import line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = load_fomc_statements()\n",
    "lm = load_loughran_mcdonald_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = statements['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = {'negative': lambda x: x.Negative>0, 'positive': lambda x: x.Positive>0,\n",
    "         'all': lambda x: x.Word.notna()}\n",
    "def get_total_count(X, lm, func):\n",
    "    m = CountVectorizer(vocabulary=lm.loc[func].Word.str.lower().values)\n",
    "    return pd.DataFrame(m.fit_transform(X).toarray(), index=X.index).sum(axis=1)\n",
    "\n",
    "lm_counts = pd.concat({k: get_total_count(X, lm, v) for k, v in funcs.items()},\n",
    "                          axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(lm_counts.pipe(lambda x:(x.positive-x.negative)/(x.positive + x.negative)).resample('B').last().ffill(),\n",
    "legend=False, title='Sentiment=(pos - neg)/(pos + neg) in FOMC statements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(lm_counts.pipe(lambda x:(x.positive-x.negative)/x['all']).resample('B').last().ffill(),\n",
    "legend=False, title='Sentiment=(pos - neg)/(all) in FOMC statements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_lexica = {'negative': pd.Series(1, lm.loc[lm.Negative>0].Word.str.lower().values),\n",
    "'positive': pd.Series(1, lm.loc[lm.Positive>0].Word.str.lower().values)}\n",
    "show_text(statements.loc[['2000-12-19', '2013-12-18', '2014-01-29']],\n",
    "lexica=lm_lexica, n=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment in FOMC statements: supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on previous analyses, we build here a `scikit-learn pipeline` with a `Tfidfvectorizer` and a regularized regression`ElasticNet`. The target is the return of the market on the day of the statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skfin.datasets import load_kf_returns\n",
    "from skfin.text import show_text\n",
    "from pandas.tseries.offsets import BDay\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = load_kf_returns(filename=\"F-F_Research_Data_Factors_daily\")['Daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_days = ['2008-01-22', '2010-05-09', '2020-03-15']\n",
    "idx0 = pd.to_datetime(pd.Index(special_days))\n",
    "idx = statements.index.difference(idx0).union(idx0 + BDay(1))\n",
    "ret_fomc = ret.div(ret.ewm(252).std()).loc[ret.index.intersection(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Pipeline([('tfidf', TfidfVectorizer(vocabulary=None,\n",
    "                                          ngram_range=(1, 3),\n",
    "                                          max_features=500,\n",
    "                                          stop_words='english',\n",
    "                                          token_pattern=r'\\b[a-zA-Z]{3,}\\b')),\n",
    "                ('reg', ElasticNet(alpha=0.0075)),])\n",
    "y = ret_fomc['Mkt-RF'].dropna()\n",
    "X = statements['text']\n",
    "idx_ = y.index.intersection(X.index)\n",
    "X, y = X.loc[idx_], y.loc[idx_]\n",
    "est.fit(X, y);\n",
    "vocab_ = pd.Series(est.named_steps['tfidf'].vocabulary_).sort_values().index\n",
    "interpret_coef = pd.DataFrame(np.transpose(est.named_steps['reg'].coef_), index=vocab_)\n",
    "coefs_plot(interpret_coef, title='Interpreted coefficients for trained model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexica = {'positive': interpret_coef.squeeze().nlargest(n=10),\n",
    "          'negative': interpret_coef.squeeze().nsmallest(n=10), }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ = pd.Series(est.predict(X), index=X.index).sort_values().pipe(lambda x: [x.index[0], x.index[-1]])\n",
    "show_text(statements.loc[idx_], lexica=lexica, n=None)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (skfin)",
   "language": "python",
   "name": "skfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
