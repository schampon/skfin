{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosted Trees and Neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we review two important classes of non-linear forecasting models: boosted trees and the multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first discuss boosted trees, in particular as described by the companion paper to the package `xgboost`: \n",
    "\n",
    "> Chen and Guestrin (2016): \"XGBoost: A Scalable Tree Boosting System,\" *Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "display(Image('images/xgboost_1.png',width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset $\\langle x_n, y_n \\rangle$ with $N$ samples ($x_n \\in \\mathbb{R}^M$), a tree ensemble model uses $K$ additive functions: \n",
    "\n",
    "$$ \\hat{y}_n = \\phi(x_n) = \\sum_k f_k(x_n),  $$ \n",
    "\n",
    "where $f_k$ is in the space of regression trees $\\mathcal {F} = \\{ f \\}$: \n",
    "\n",
    "- $q$: $\\mathbb{R}^M \\rightarrow J$ is a partition; \n",
    "- $f(x) = w_{q(x)}$ is a constant value on each leaf of the tree. \n",
    "\n",
    "The objective is to minimize the loss: \n",
    "\n",
    "$$ \\mathcal{L}(\\phi) = \\sum_n l(y_n, \\hat{y}_n) + \\sum_k \\Omega(f_k),$$\n",
    "\n",
    "where $\\Omega(f) = \\gamma J + \\frac{1}{2}\\lambda || w ||^2$ is a regularisation term and $l$ is a convex loss function (e.g. mean squared error). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions $f_k$ are derived iteratively: \n",
    "\n",
    "$$ \\mathcal {L}^k =  \\sum_n l \\left(y_n, \\hat{y}^{k-1}_n + f_k(x_n) \\right) + \\Omega (f_k).$$\n",
    "\n",
    "With a second-order Taylor expansion, we have \n",
    "\n",
    "$$ \\mathcal {L}^k \\approx \\sum_n \\left[ l (y_n, \\hat{y}^{k-1}_n) + g_n f_k(x_n) + \\frac{1}{2} h_n f_k(x_n)^2 \\right] + \\Omega (f_k), $$\n",
    "\n",
    "where $g_n = \\partial_{\\hat{y}} l(y_n, \\hat{y}^{k-1}_n)$ and $h_n = \\partial^2 _{\\hat{y}} l(y_n, \\hat{y}^{k-1}_n)$. For an instance of leaf $j$, with $I_j = \\{n| q(x_n)= j \\}$, we can sum by leaf: \n",
    "\n",
    "$$ \\mathcal {L}^{k} = \\sum_{j=1}^{j=J} \\left(\\sum_{n \\in I_j} g_n \\right) w_j + \\frac{1}{2} \\left(\\sum_{n \\in I_j} h_n + \\lambda \\right) w_j^2 + \\gamma J + constant. $$\n",
    "\n",
    "For a given $q(x)$, the optimal weight $w_j^*$ of leaf $j$ is \n",
    "\n",
    "$$ w^*_j = - \\frac{ \\sum_{n \\in I_j} g_n }{\\sum_{n \\in I_j} h_n + \\lambda}. $$\n",
    "\n",
    "The corresponding optimal value is then\n",
    "\n",
    "$$\\tilde{\\mathcal{L}}^k (q) = - \\frac{1}{2} \\sum_{j=1}^{j=J} \\frac{\\left( \\sum_{n \\in I_j} g_n \\right)^2 }{\\sum_{n \\in I_j} h_n + \\lambda} + \\gamma J + constant. $$\n",
    "\n",
    "A greedy algorithm that starts from a single leaf and iteratively adds branches to the tree is used to dermine $q$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "display(Image('images/xgboost_3.png',width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, `xgboost` (and `lightgbm`) can be used with custom loss functions -- for instance, by providing the gradient and hessian functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input $x \\in \\mathbb{M}$, the layer (with hidden size equals to $K$) of a multi-layer perceptron is given by \n",
    "\n",
    "$$g(b + W x)$$\n",
    "\n",
    "where $W$ is a $[K \\times M]$ matrix, $b$ is a scalar and $g$ is an activation function. The output of the last layer has to match the size of the target vector $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting industry returns with non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfin.plot import line, heatmap\n",
    "from skfin.metrics import sharpe_ratio\n",
    "from skfin.datasets import load_kf_returns\n",
    "from skfin.backtesting import Backtester\n",
    "from skfin.mv_estimators import MeanVariance\n",
    "from skfin.estimators import Ridge, MLPRegressor, MultiOutputRegressor\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm.sklearn import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_data = load_kf_returns(cache_dir='data')\n",
    "ret = returns_data['Monthly']['Average_Value_Weighted_Returns'][:'1999']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_X = lambda x: x.rolling(12).mean().fillna(0).values\n",
    "transform_y = lambda x: x.shift(-1).values\n",
    "features = transform_X(ret)\n",
    "target = transform_y(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a benchmark based on estimating the cross-industry effects, we first look at the `Ridge` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator0 = make_pipeline(StandardScaler(with_mean=False), \n",
    "                           Ridge(), \n",
    "                           MeanVariance())\n",
    "m = Backtester(estimator0, ret).train(features, target)\n",
    "h0, pnl0, estimators0 = m.h_, m.pnl_, m.estimators_\n",
    "pnls = {'Ridge (benchmark)': pnl0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pnl0, cumsum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(MultiOutputRegressor(LGBMRegressor(min_child_samples=5, \n",
    "                                                             n_estimators=25, n_jobs=1)), \n",
    "                          MeanVariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "m = Backtester(estimator, ret).train(features, target)\n",
    "pnls['lightgbm'] = m.pnl_\n",
    "line(pnls, cumsum=True, title='Lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: np.stack([m.booster_.feature_importance(importance_type='gain') for m in x])\n",
    "importance = [func(m_[0].estimators_) for m_ in m.estimators_]\n",
    "importance_mean = pd.DataFrame(sum(importance)/len(importance), ret.columns, ret.columns).T\n",
    "\n",
    "heatmap(importance_mean.loc[importance_mean.mean().sort_values().index, importance_mean.mean().sort_values().index],\n",
    "       title='Average feature importance: gain (x-axis: predictors, y-axis=targets)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPRegressor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Inner workings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first focus on a single window to understand how the `MLPRegressor` works in `scikit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in m.cv.split(ret): \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True)\n",
    "X_train = scaler.fit_transform(features[train])\n",
    "y_train = target[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(features[test])\n",
    "y_test = target[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a simple `MLP` with 6 neurons. The activation function is a logistic sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MLPRegressor(hidden_layer_sizes=(6,), \n",
    "                 solver='adam', \n",
    "                 learning_rate_init=0.5,\n",
    "                 alpha=100, \n",
    "                 activation='logistic',\n",
    "                 tol=1e-2,\n",
    "                 n_iter_no_change = 25,\n",
    "                 early_stopping=False)\n",
    "m.fit(X_train, y_train)\n",
    "y_pred = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `early_stopping` is `False`, the optimisation stops based on the in-sample score, while when `early_stopping` is `True`, \n",
    "the decision to stop is based on a radom sample (e.g. 10% of the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of iterations is {m.n_iter_} (out of a maximum of {m.max_iter}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of parameter:\\n - first layer: {12 * 6  + 6}\\n - second layer: {12 * 6 + 12}\\n - total number of parameters: {12  * 12 + 6 + 12}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.coefs_[0].shape, m.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.intercepts_[0].shape, m.intercepts_[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sigmoid logistic` activation function is also known as `expit` and it is provided by the `scipy` package. \n",
    "\n",
    "The MLP regressor in this case: \n",
    "- project the vector of size 12 on a vector of size 6\n",
    "- a bias vector of size 6 is added\n",
    "- the activitation function (here the `sigmoid`) regularizes the neurons\n",
    "- the second layer then projects the vector of size 6 on a vector of size 12 (with a bias of size 12). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "y_pred_ = expit(X_test.dot(m.coefs_[0]) + m.intercepts_[0]).dot(m.coefs_[1]) + m.intercepts_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(y_pred, y_pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn` package provides a loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pd.Series(m.loss_curve_), legend=False, title='Loss curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic loss of is the `squared error`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(m.loss_curve_[-1],  ((y_train - y_pred) ** 2).mean() / 2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backtest with MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at a backtest using `MLPRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "estimator = make_pipeline(StandardScaler(with_mean=False), \n",
    "                          MLPRegressor(hidden_layer_sizes=(6,), \n",
    "                                       learning_rate_init=0.5,\n",
    "                                       alpha = 100, \n",
    "                                       solver='adam',\n",
    "                                       activation='logistic', \n",
    "                                       tol=1e-2,\n",
    "                                       n_iter_no_change = 25,\n",
    "                                       early_stopping=False), \n",
    "                          MeanVariance())\n",
    "m = Backtester(estimator, ret).train(features, target)\n",
    "pnls['mlp'] = m.pnl_\n",
    "\n",
    "line(pnls, cumsum=True, title='mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pd.Series([m_[1].n_iter_ for m_ in m.estimators_]), \n",
    "     title='Number of iterations', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for m_ in m.estimators_: \n",
    "    break\n",
    "    \n",
    "heatmap(pd.DataFrame(m_[1].coefs_[0], index=ret.columns), title='First estimator: first layer coefficients')\n",
    "heatmap(pd.DataFrame(m_[1].coefs_[1], columns = ret.columns), title='First estimator: second layer coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the stochasticity of the estimation, we are interested in evaluated the noise associated to a given run. To assess, we re-run the backtest with exactly the same estimator (and hence the same parameters). In fact, such stochasticity depends on the amount regularisation, and to make this point, we relax it with `alpha=50` (instead of 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "estimator_ = make_pipeline(StandardScaler(with_mean=False), \n",
    "                      MLPRegressor(hidden_layer_sizes=(6,), \n",
    "                                   learning_rate_init=0.5,\n",
    "                                   alpha = 50, \n",
    "                                   n_iter_no_change = 25,\n",
    "                                   solver='adam', \n",
    "                                   tol=1e-2,\n",
    "                                   activation='logistic'), \n",
    "                      MeanVariance())\n",
    "\n",
    "n_iter = 10\n",
    "pnls_ = {}\n",
    "for i in range(n_iter): \n",
    "    pnls_[i] = Backtester(estimator_, ret).train(features, target).pnl_\n",
    "\n",
    "sr_std = np.std([v.pipe(sharpe_ratio) for k, v in pnls_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'MLP (n_iter={n_iter}, sr std= {sr_std:.2f})'\n",
    "line(pd.concat(pnls_, axis=1).assign(mean = lambda x: x.mean(axis=1)), cumsum=True, title=title)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (skfin)",
   "language": "python",
   "name": "skfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
