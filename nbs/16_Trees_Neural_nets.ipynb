{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d3ff11",
   "metadata": {},
   "source": [
    "# Non-Linear Forecasting Models: Boosted Trees and Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a1c8c",
   "metadata": {},
   "source": [
    "In this section, we review two important classes of non-linear forecasting models: boosted trees and the multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e270470",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:39:44.631787Z",
     "iopub.status.busy": "2025-09-29T15:39:44.631555Z",
     "iopub.status.idle": "2025-09-29T15:39:46.476755Z",
     "shell.execute_reply": "2025-09-29T15:39:46.476168Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "from skfin.backtesting import Backtester\n",
    "from skfin.datasets_ import load_kf_returns\n",
    "from skfin.metrics import sharpe_ratio\n",
    "from skfin.mv_estimators import MeanVariance\n",
    "from skfin.plot import heatmap, line\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "returns_data = load_kf_returns(cache_dir=\"data\")\n",
    "ret = returns_data[\"Monthly\"][\"Average_Value_Weighted_Returns\"][:\"1999\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd4967",
   "metadata": {},
   "source": [
    "## Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0756d036",
   "metadata": {},
   "source": [
    "We first discuss boosted trees, in particular as described by the companion paper to the package `xgboost`: \n",
    "\n",
    "> Chen and Guestrin (2016): \"XGBoost: A Scalable Tree Boosting System,\" *Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cceb7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:39:46.479071Z",
     "iopub.status.busy": "2025-09-29T15:39:46.478706Z",
     "iopub.status.idle": "2025-09-29T15:39:46.505860Z",
     "shell.execute_reply": "2025-09-29T15:39:46.505413Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "display(Image(\"images/xgboost_1.png\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cae3e1",
   "metadata": {},
   "source": [
    "For a dataset $\\langle x_n, y_n \\rangle$ with $N$ samples ($x_n \\in \\mathbb{R}^M$), a tree ensemble model uses $K$ additive functions: \n",
    "\n",
    "$$ \\hat{y}_n = \\phi(x_n) = \\sum_k f_k(x_n),  $$ \n",
    "\n",
    "where $f_k$ is in the space of regression trees $\\mathcal {F} = \\{ f \\}$: \n",
    "\n",
    "- $q$: $\\mathbb{R}^M \\rightarrow J$ is a partition; \n",
    "- $f(x) = w_{q(x)}$ is a constant value on each leaf of the tree. \n",
    "\n",
    "The objective is to minimize the loss: \n",
    "\n",
    "$$ \\mathcal{L}(\\phi) = \\sum_n l(y_n, \\hat{y}_n) + \\sum_k \\Omega(f_k),$$\n",
    "\n",
    "where $\\Omega(f) = \\gamma J + \\frac{1}{2}\\lambda || w ||^2$ is a regularisation term and $l$ is a convex loss function (e.g. mean squared error). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd8862e",
   "metadata": {},
   "source": [
    "The functions $f_k$ are derived iteratively: \n",
    "\n",
    "$$ \\mathcal {L}^k =  \\sum_n l \\left(y_n, \\hat{y}^{k-1}_n + f_k(x_n) \\right) + \\Omega (f_k).$$\n",
    "\n",
    "With a second-order Taylor expansion, we have \n",
    "\n",
    "$$ \\mathcal {L}^k \\approx \\sum_n \\left[ l (y_n, \\hat{y}^{k-1}_n) + g_n f_k(x_n) + \\frac{1}{2} h_n f_k(x_n)^2 \\right] + \\Omega (f_k), $$\n",
    "\n",
    "where $g_n = \\partial_{\\hat{y}} l(y_n, \\hat{y}^{k-1}_n)$ and $h_n = \\partial^2 _{\\hat{y}} l(y_n, \\hat{y}^{k-1}_n)$. For an instance of leaf $j$, with $I_j = \\{n| q(x_n)= j \\}$, we can sum by leaf: \n",
    "\n",
    "$$ \\mathcal {L}^{k} = \\sum_{j=1}^{j=J} \\left(\\sum_{n \\in I_j} g_n \\right) w_j + \\frac{1}{2} \\left(\\sum_{n \\in I_j} h_n + \\lambda \\right) w_j^2 + \\gamma J + constant. $$\n",
    "\n",
    "For a given $q(x)$, the optimal weight $w_j^*$ of leaf $j$ is \n",
    "\n",
    "$$ w^*_j = - \\frac{ \\sum_{n \\in I_j} g_n }{\\sum_{n \\in I_j} h_n + \\lambda}. $$\n",
    "\n",
    "The corresponding optimal value is then\n",
    "\n",
    "$$\\tilde{\\mathcal{L}}^k (q) = - \\frac{1}{2} \\sum_{j=1}^{j=J} \\frac{\\left( \\sum_{n \\in I_j} g_n \\right)^2 }{\\sum_{n \\in I_j} h_n + \\lambda} + \\gamma J + constant. $$\n",
    "\n",
    "A greedy algorithm that starts from a single leaf and iteratively adds branches to the tree is used to dermine $q$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d72bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:39:46.507597Z",
     "iopub.status.busy": "2025-09-29T15:39:46.507372Z",
     "iopub.status.idle": "2025-09-29T15:39:46.536046Z",
     "shell.execute_reply": "2025-09-29T15:39:46.535602Z"
    }
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "display(Image(\"images/xgboost_3.png\", width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2c936",
   "metadata": {},
   "source": [
    "In practice, `xgboost` (and `lightgbm`) can be used with custom loss functions -- for instance, by providing the gradient and hessian functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb1432",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7aa5ca",
   "metadata": {},
   "source": [
    "For the input $x \\in \\mathbb{M}$, the layer (with hidden size equals to $K$) of a multi-layer perceptron is given by \n",
    "\n",
    "$$g(b + W x)$$\n",
    "\n",
    "where $W$ is a $[K \\times M]$ matrix, $b$ is a scalar and $g$ is an activation function. The output of the last layer has to match the size of the target vector $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c77385",
   "metadata": {},
   "source": [
    "## Predicting industry returns with non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf8d92",
   "metadata": {},
   "source": [
    "### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f4d36-ff2f-4ee5-a01b-76c38cb6e594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:39:46.537816Z",
     "iopub.status.busy": "2025-09-29T15:39:46.537608Z",
     "iopub.status.idle": "2025-09-29T15:39:46.563078Z",
     "shell.execute_reply": "2025-09-29T15:39:46.562612Z"
    }
   },
   "outputs": [],
   "source": [
    "from skfin.estimators import MLPRegressor, MultiLGBMRegressor, Ridge\n",
    "\n",
    "\n",
    "def transform_X(df, window=12):\n",
    "    return df.rolling(window=window).mean()\n",
    "\n",
    "\n",
    "def transform_y(df):\n",
    "    return df.shift(-1)\n",
    "\n",
    "\n",
    "features = transform_X(ret)\n",
    "target = transform_y(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925dd5d",
   "metadata": {},
   "source": [
    "As a benchmark based on estimating the cross-industry effects, we first look at the `Ridge` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf08b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:39:46.564717Z",
     "iopub.status.busy": "2025-09-29T15:39:46.564491Z",
     "iopub.status.idle": "2025-09-29T15:39:47.568389Z",
     "shell.execute_reply": "2025-09-29T15:39:47.567815Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator0 = make_pipeline(StandardScaler(with_mean=False), Ridge(), MeanVariance())\n",
    "m = (\n",
    "    Backtester(estimator0, name=\"Ridge (benchmark)\")\n",
    "    .compute_holdings(features, target)\n",
    "    .compute_pnl(ret)\n",
    ")\n",
    "h0, pnl0, estimators0 = m.h_, m.pnl_, m.estimators_\n",
    "pnls = [pnl0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ed81f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:39:47.570340Z",
     "iopub.status.busy": "2025-09-29T15:39:47.570065Z",
     "iopub.status.idle": "2025-09-29T15:39:47.599274Z",
     "shell.execute_reply": "2025-09-29T15:39:47.598771Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator = make_pipeline(\n",
    "    MultiLGBMRegressor(min_child_samples=5, n_estimators=25), MeanVariance()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a144403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:39:47.601146Z",
     "iopub.status.busy": "2025-09-29T15:39:47.600894Z",
     "iopub.status.idle": "2025-09-29T15:40:30.024167Z",
     "shell.execute_reply": "2025-09-29T15:40:30.023633Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "m = (\n",
    "    Backtester(estimator, name=\"lightgm\")\n",
    "    .compute_holdings(features, target)\n",
    "    .compute_pnl(ret)\n",
    ")\n",
    "pnls += [m.pnl_]\n",
    "line(pnls, cumsum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10ff2a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.026032Z",
     "iopub.status.busy": "2025-09-29T15:40:30.025762Z",
     "iopub.status.idle": "2025-09-29T15:40:30.326382Z",
     "shell.execute_reply": "2025-09-29T15:40:30.325867Z"
    }
   },
   "outputs": [],
   "source": [
    "func = lambda x: np.stack(\n",
    "    [m.booster_.feature_importance(importance_type=\"gain\") for m in x]\n",
    ")\n",
    "importance = [func(m_.steps[0][1].m.estimators_) for m_ in m.estimators_]\n",
    "importance_mean = pd.DataFrame(\n",
    "    sum(importance) / len(importance), ret.columns, ret.columns\n",
    ").T\n",
    "\n",
    "heatmap(\n",
    "    importance_mean.loc[\n",
    "        importance_mean.mean().sort_values().index,\n",
    "        importance_mean.mean().sort_values().index,\n",
    "    ],\n",
    "    title=\"Average feature importance: gain (x-axis: predictors, y-axis=targets)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621a619",
   "metadata": {},
   "source": [
    "### MLPRegressor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83daa2cb",
   "metadata": {},
   "source": [
    "We first focus on a single window to understand how the `MLPRegressor` works in `scikit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46654469",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.328185Z",
     "iopub.status.busy": "2025-09-29T15:40:30.327937Z",
     "iopub.status.idle": "2025-09-29T15:40:30.353034Z",
     "shell.execute_reply": "2025-09-29T15:40:30.352552Z"
    }
   },
   "outputs": [],
   "source": [
    "for train, test in m.cv_.split(ret):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ee621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.354647Z",
     "iopub.status.busy": "2025-09-29T15:40:30.354434Z",
     "iopub.status.idle": "2025-09-29T15:40:30.381969Z",
     "shell.execute_reply": "2025-09-29T15:40:30.381501Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True)\n",
    "X_train = scaler.fit_transform(features.iloc[train])\n",
    "y_train = target.iloc[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9a5d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.383573Z",
     "iopub.status.busy": "2025-09-29T15:40:30.383351Z",
     "iopub.status.idle": "2025-09-29T15:40:30.409044Z",
     "shell.execute_reply": "2025-09-29T15:40:30.408547Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = scaler.transform(features.iloc[test])\n",
    "y_test = target.iloc[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89f82d",
   "metadata": {},
   "source": [
    "We instantiate a simple `MLP` with 6 neurons. The activation function is a logistic sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddee056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.410795Z",
     "iopub.status.busy": "2025-09-29T15:40:30.410561Z",
     "iopub.status.idle": "2025-09-29T15:40:30.836747Z",
     "shell.execute_reply": "2025-09-29T15:40:30.836193Z"
    }
   },
   "outputs": [],
   "source": [
    "m = MLPRegressor(\n",
    "    hidden_layer_sizes=(6,),\n",
    "    solver=\"adam\",\n",
    "    learning_rate_init=0.5,\n",
    "    alpha=100,\n",
    "    activation=\"logistic\",\n",
    "    tol=1e-2,\n",
    "    n_iter_no_change=25,\n",
    "    early_stopping=False,\n",
    ")\n",
    "m.fit(X_train, y_train)\n",
    "y_pred = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56072a",
   "metadata": {},
   "source": [
    "When `early_stopping` is `False`, the optimisation stops based on the in-sample score, while when `early_stopping` is `True`, \n",
    "the decision to stop is based on a random sample (e.g. 10% of the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0008b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.838544Z",
     "iopub.status.busy": "2025-09-29T15:40:30.838282Z",
     "iopub.status.idle": "2025-09-29T15:40:30.863515Z",
     "shell.execute_reply": "2025-09-29T15:40:30.863019Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The number of iterations is {m.n_iter_} (out of a maximum of {m.max_iter}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f842e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.865218Z",
     "iopub.status.busy": "2025-09-29T15:40:30.864922Z",
     "iopub.status.idle": "2025-09-29T15:40:30.889834Z",
     "shell.execute_reply": "2025-09-29T15:40:30.889356Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Number of parameter:\\n - first layer: {12 * 6 + 6}\\n - second layer: {12 * 6 + 12}\\n - total number of parameters: {12 * 12 + 6 + 12}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c666952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.891466Z",
     "iopub.status.busy": "2025-09-29T15:40:30.891243Z",
     "iopub.status.idle": "2025-09-29T15:40:30.916953Z",
     "shell.execute_reply": "2025-09-29T15:40:30.916490Z"
    }
   },
   "outputs": [],
   "source": [
    "m.coefs_[0].shape, m.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69841e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.918573Z",
     "iopub.status.busy": "2025-09-29T15:40:30.918358Z",
     "iopub.status.idle": "2025-09-29T15:40:30.943460Z",
     "shell.execute_reply": "2025-09-29T15:40:30.943003Z"
    }
   },
   "outputs": [],
   "source": [
    "m.intercepts_[0].shape, m.intercepts_[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b685f0",
   "metadata": {},
   "source": [
    "The `sigmoid logistic` activation function is also known as `expit` and it is provided by the `scipy` package. \n",
    "\n",
    "The MLP regressor in this case: \n",
    "\n",
    "- project the vector of size 12 on a vector of size 6\n",
    "- a bias vector of size 6 is added\n",
    "- the activitation function (here the `sigmoid`) regularizes the neurons\n",
    "- the second layer then projects the vector of size 6 on a vector of size 12 (with a bias of size 12). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93d4f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.945119Z",
     "iopub.status.busy": "2025-09-29T15:40:30.944889Z",
     "iopub.status.idle": "2025-09-29T15:40:30.969481Z",
     "shell.execute_reply": "2025-09-29T15:40:30.968942Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "y_pred_ = (\n",
    "    expit(X_test.dot(m.coefs_[0]) + m.intercepts_[0]).dot(m.coefs_[1])\n",
    "    + m.intercepts_[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6c3da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.971182Z",
     "iopub.status.busy": "2025-09-29T15:40:30.970955Z",
     "iopub.status.idle": "2025-09-29T15:40:30.998272Z",
     "shell.execute_reply": "2025-09-29T15:40:30.997796Z"
    }
   },
   "outputs": [],
   "source": [
    "np.allclose(y_pred, y_pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182dca0",
   "metadata": {},
   "source": [
    "The `sklearn` package provides a loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87ae15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:30.999942Z",
     "iopub.status.busy": "2025-09-29T15:40:30.999712Z",
     "iopub.status.idle": "2025-09-29T15:40:31.115352Z",
     "shell.execute_reply": "2025-09-29T15:40:31.114794Z"
    }
   },
   "outputs": [],
   "source": [
    "line(pd.Series(m.loss_curve_), legend=False, title=\"Loss curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3179eae",
   "metadata": {},
   "source": [
    "The quadratic loss of is the `squared error`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529b98b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:31.117131Z",
     "iopub.status.busy": "2025-09-29T15:40:31.116877Z",
     "iopub.status.idle": "2025-09-29T15:40:31.147080Z",
     "shell.execute_reply": "2025-09-29T15:40:31.146596Z"
    }
   },
   "outputs": [],
   "source": [
    "np.allclose(m.loss_curve_[-1], ((y_train - y_pred) ** 2).mean().mean() / 2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b39b2c",
   "metadata": {},
   "source": [
    "We now look at a backtest using `MLPRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06094bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:31.148832Z",
     "iopub.status.busy": "2025-09-29T15:40:31.148562Z",
     "iopub.status.idle": "2025-09-29T15:40:44.049249Z",
     "shell.execute_reply": "2025-09-29T15:40:44.048728Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator = make_pipeline(\n",
    "    StandardScaler(with_mean=False),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(6,),\n",
    "        learning_rate_init=0.5,\n",
    "        alpha=100,\n",
    "        solver=\"adam\",\n",
    "        activation=\"logistic\",\n",
    "        tol=1e-2,\n",
    "        n_iter_no_change=25,\n",
    "        early_stopping=False,\n",
    "    ),\n",
    "    MeanVariance(),\n",
    ")\n",
    "m = (\n",
    "    Backtester(estimator, name=\"MLP\")\n",
    "    .compute_holdings(features, target)\n",
    "    .compute_pnl(ret)\n",
    ")\n",
    "pnls += [m.pnl_]\n",
    "\n",
    "line(pnls, cumsum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94083641-f9bf-4d94-81ca-b183b2f63ba2",
   "metadata": {},
   "source": [
    "The graph below shows the number of iteration in the `Adam` stochastic gradient descent of the MLP estimator. While the MLP has no fixed number of iterations, it oscillates around 80. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04516b77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:44.051228Z",
     "iopub.status.busy": "2025-09-29T15:40:44.050952Z",
     "iopub.status.idle": "2025-09-29T15:40:44.328846Z",
     "shell.execute_reply": "2025-09-29T15:40:44.328368Z"
    }
   },
   "outputs": [],
   "source": [
    "line(\n",
    "    pd.Series([m_[1].n_iter_ for m_ in m.estimators_]),\n",
    "    title=\"Number of iterations\",\n",
    "    legend=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1aea9-1885-44c8-a1f8-73b562360e51",
   "metadata": {},
   "source": [
    "The heatmaps below shows the coefficients $W$ for the first and second layers of the MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c55c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:44.330701Z",
     "iopub.status.busy": "2025-09-29T15:40:44.330470Z",
     "iopub.status.idle": "2025-09-29T15:40:44.638987Z",
     "shell.execute_reply": "2025-09-29T15:40:44.638466Z"
    }
   },
   "outputs": [],
   "source": [
    "for m_ in m.estimators_:\n",
    "    break\n",
    "\n",
    "heatmap(\n",
    "    pd.DataFrame(m_[1].coefs_[0], index=ret.columns),\n",
    "    title=\"First estimator: first layer coefficients\",\n",
    ")\n",
    "heatmap(\n",
    "    pd.DataFrame(m_[1].coefs_[1], columns=ret.columns),\n",
    "    title=\"First estimator: second layer coefficients\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d664d",
   "metadata": {},
   "source": [
    "Given the stochasticity of the estimation, we are interested in evaluating the noise associated to a given run. More precisely, we re-run the backtest with exactly the same estimator (and hence the same parameters). In fact, such stochasticity depends on the amount regularisation, and to make this point, we relax it with `alpha=50` (instead of 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe71d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:40:44.640856Z",
     "iopub.status.busy": "2025-09-29T15:40:44.640622Z",
     "iopub.status.idle": "2025-09-29T15:42:30.086523Z",
     "shell.execute_reply": "2025-09-29T15:42:30.085988Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator_ = make_pipeline(\n",
    "    StandardScaler(with_mean=False),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(6,),\n",
    "        learning_rate_init=0.5,\n",
    "        alpha=50,\n",
    "        n_iter_no_change=25,\n",
    "        solver=\"adam\",\n",
    "        tol=1e-2,\n",
    "        activation=\"logistic\",\n",
    "    ),\n",
    "    MeanVariance(),\n",
    ")\n",
    "n_iter = 10\n",
    "pnls_ = {}\n",
    "for i in range(n_iter):\n",
    "    pnls_[i] = Backtester(estimator_).train(features, target, ret)\n",
    "\n",
    "sr_std = np.std([v.pipe(sharpe_ratio) for k, v in pnls_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64584cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T15:42:30.088380Z",
     "iopub.status.busy": "2025-09-29T15:42:30.088122Z",
     "iopub.status.idle": "2025-09-29T15:42:31.888719Z",
     "shell.execute_reply": "2025-09-29T15:42:31.888187Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "title = f\"MLP (n_iter={n_iter}, sr std= {sr_std:.2f})\"\n",
    "line(\n",
    "    pd.concat(pnls_, axis=1).assign(mean=lambda x: x.mean(axis=1)),\n",
    "    cumsum=True,\n",
    "    title=title,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "skfin 2024",
   "language": "python",
   "name": "skfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
